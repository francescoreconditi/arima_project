{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üß™ Ambiente R&D: Algoritmi Avanzati di Forecasting per Serie Temporali\n\n**Notebook di Ricerca e Sviluppo**  \n*Ambiente professionale per sperimentare algoritmi di forecasting all'avanguardia*\n\n---\n\n## üìã Obiettivi di Ricerca\n\nQuesto notebook serve come ambiente R&D completo per:\n\n1. **üî¨ Sperimentazione Algoritmica**: Testare e confrontare nuovi metodi di forecasting\n2. **üìä Benchmarking delle Performance**: Valutazione rigorosa attraverso metriche multiple\n3. **üéØ Sviluppo di Metodi**: Prototipare e validare approcci innovativi\n4. **üìà Ricerca su Modelli Ibridi**: Combinare diverse tecniche per prestazioni migliorate\n5. **üîç Analisi Approfondita**: Capire quando e perch√© diversi metodi funzionano\n\n---\n\n## üèóÔ∏è Framework di Ricerca\n\n### Algoritmi da Esplorare:\n- **Classici**: ARIMA, SARIMA, Exponential Smoothing\n- **Machine Learning**: Random Forest, XGBoost, SVR\n- **Deep Learning**: LSTM, GRU, modelli basati su Transformer\n- **Metodi Ensemble**: Media pesata, stacking, blending\n- **Approcci Ibridi**: Combinazioni Statistiche + ML\n\n### Framework di Valutazione:\n- **Metriche di Accuratezza**: MAE, RMSE, MAPE, SMAPE\n- **Test Statistici**: Diebold-Mariano, Model Confidence Set\n- **Robustezza**: Performance attraverso diverse caratteristiche dei dati\n- **Efficienza Computazionale**: Tempo di training, utilizzo memoria\n- **Interpretabilit√†**: Spiegabilit√† del modello e importanza delle feature"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üîß Setup e Import\n\nInizializza l'ambiente di ricerca con tutte le librerie e configurazioni necessarie."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Librerie core\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pathlib import Path\nimport sys\nimport time\nfrom typing import Dict, List, Tuple, Any, Optional, Union\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport pickle\nimport json\n\n# Statistiche e serie temporali\nfrom scipy import stats\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# Librerie ML avanzate (installa se necessario)\ntry:\n    import xgboost as xgb\n    XGBOOST_AVAILABLE = True\nexcept ImportError:\n    print(\"‚ö†Ô∏è  XGBoost non disponibile. Installa con: pip install xgboost\")\n    XGBOOST_AVAILABLE = False\n\ntry:\n    from sklearn.svm import SVR\n    SVR_AVAILABLE = True\nexcept ImportError:\n    SVR_AVAILABLE = False\n\n# Aggiungi percorso progetto per import\nproject_root = Path().resolve()\nif 'notebooks' in str(project_root):\n    project_root = project_root.parent\nsys.path.insert(0, str(project_root / 'src'))\n\n# Import del progetto\ntry:\n    from arima_forecaster import ARIMAForecaster, SARIMAForecaster\n    from arima_forecaster.data import DataLoader, TimeSeriesPreprocessor\n    from arima_forecaster.evaluation import ModelEvaluator\n    from arima_forecaster.visualization import ForecastPlotter\n    from arima_forecaster.core import ARIMAModelSelector, SARIMAModelSelector\n    from arima_forecaster.utils import setup_logger\n    PROJECT_IMPORTS_OK = True\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è  Import del progetto non disponibili: {e}\")\n    print(\"Esegui questo notebook dalla root del progetto o installa il package in modalit√† sviluppo\")\n    PROJECT_IMPORTS_OK = False\n\n# Configurazione\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\n# Configurazione ricerca\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\n# Crea directory di output\noutput_dir = project_root / \"outputs\" / \"research\"\noutput_dir.mkdir(parents=True, exist_ok=True)\nplots_dir = output_dir / \"plots\"\nplots_dir.mkdir(exist_ok=True)\nmodels_dir = output_dir / \"models\"\nmodels_dir.mkdir(exist_ok=True)\n\nprint(\"üî¨ Ambiente R&D inizializzato con successo!\")\nprint(f\"üìÅ Directory output: {output_dir}\")\nprint(f\"üéØ Random seed: {RANDOM_SEED}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üèóÔ∏è Infrastruttura di Ricerca\n\nDefinisce le classi e funzioni core per la sperimentazione algoritmica sistematica."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass ExperimentResult:\n    \"\"\"Contenitore per i risultati degli esperimenti.\"\"\"\n    algorithm_name: str\n    parameters: Dict[str, Any]\n    training_time: float\n    prediction_time: float\n    memory_usage: float\n    metrics: Dict[str, float]\n    predictions: np.ndarray\n    model_object: Any = None\n    feature_importance: Optional[Dict[str, float]] = None\n    confidence_intervals: Optional[np.ndarray] = None\n    \nclass AlgorithmBenchmark:\n    \"\"\"Sistema di benchmarking professionale per algoritmi di serie temporali.\"\"\"\n    \n    def __init__(self, random_seed: int = 42):\n        self.random_seed = random_seed\n        self.results: List[ExperimentResult] = []\n        self.datasets: Dict[str, pd.Series] = {}\n        \n    def add_dataset(self, name: str, data: pd.Series, description: str = \"\"):\n        \"\"\"Aggiungi un dataset per il benchmarking.\"\"\"\n        self.datasets[name] = {\n            'data': data,\n            'description': description,\n            'length': len(data),\n            'frequency': data.index.freq if hasattr(data.index, 'freq') else 'Sconosciuta'\n        }\n        print(f\"üìä Dataset '{name}' aggiunto: {len(data)} osservazioni\")\n    \n    def evaluate_model(self, y_true: np.ndarray, y_pred: np.ndarray, \n                      y_pred_ci: Optional[np.ndarray] = None) -> Dict[str, float]:\n        \"\"\"Metriche complete di valutazione del modello.\"\"\"\n        metrics = {}\n        \n        # Metriche di accuratezza base\n        metrics['MAE'] = mean_absolute_error(y_true, y_pred)\n        metrics['RMSE'] = np.sqrt(mean_squared_error(y_true, y_pred))\n        metrics['MAPE'] = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n        \n        # MAPE simmetrico (gestisce meglio i valori zero)\n        metrics['SMAPE'] = np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100\n        \n        # R-squared (coefficiente di determinazione)\n        metrics['R2'] = r2_score(y_true, y_pred)\n        \n        # Accuratezza direzionale (per predizione trend)\n        if len(y_true) > 1:\n            true_directions = np.diff(y_true) > 0\n            pred_directions = np.diff(y_pred) > 0\n            metrics['Directional_Accuracy'] = np.mean(true_directions == pred_directions) * 100\n        \n        # Statistica U di Theil (accuratezza forecast relativa a forecast na√Øve)\n        naive_forecast = y_true[:-1]  # forecast lag-1\n        if len(naive_forecast) > 0:\n            mse_model = mean_squared_error(y_true[1:], y_pred[1:])\n            mse_naive = mean_squared_error(y_true[1:], naive_forecast)\n            metrics['Theil_U'] = np.sqrt(mse_model / mse_naive) if mse_naive > 0 else np.inf\n        \n        # Rapporto di copertura per intervalli di confidenza\n        if y_pred_ci is not None and len(y_pred_ci.shape) == 2:\n            lower_bound = y_pred_ci[:, 0]\n            upper_bound = y_pred_ci[:, 1]\n            coverage = np.mean((y_true >= lower_bound) & (y_true <= upper_bound))\n            metrics['CI_Coverage'] = coverage * 100\n            \n            # Larghezza media dell'intervallo\n            metrics['CI_Width'] = np.mean(upper_bound - lower_bound)\n        \n        return metrics\n    \n    def run_experiment(self, algorithm_func, algorithm_name: str, \n                      train_data: pd.Series, test_data: pd.Series,\n                      **algorithm_params) -> ExperimentResult:\n        \"\"\"Esegui un singolo esperimento algoritmico.\"\"\"\n        \n        print(f\"üî¨ Eseguendo esperimento: {algorithm_name}\")\n        \n        # Misura tempo di training e memoria\n        import psutil\n        process = psutil.Process()\n        mem_before = process.memory_info().rss / 1024 / 1024  # MB\n        \n        start_time = time.time()\n        \n        try:\n            # Addestra modello\n            model, model_info = algorithm_func(train_data, **algorithm_params)\n            training_time = time.time() - start_time\n            \n            # Misura tempo di predizione\n            pred_start = time.time()\n            predictions, confidence_intervals = self._get_predictions(model, len(test_data), model_info)\n            prediction_time = time.time() - pred_start\n            \n            # Utilizzo memoria\n            mem_after = process.memory_info().rss / 1024 / 1024\n            memory_usage = mem_after - mem_before\n            \n            # Valuta performance\n            metrics = self.evaluate_model(test_data.values, predictions, confidence_intervals)\n            \n            # Importanza feature (se disponibile)\n            feature_importance = self._extract_feature_importance(model, model_info)\n            \n            result = ExperimentResult(\n                algorithm_name=algorithm_name,\n                parameters=algorithm_params,\n                training_time=training_time,\n                prediction_time=prediction_time,\n                memory_usage=memory_usage,\n                metrics=metrics,\n                predictions=predictions,\n                model_object=model,\n                feature_importance=feature_importance,\n                confidence_intervals=confidence_intervals\n            )\n            \n            self.results.append(result)\n            print(f\"‚úÖ {algorithm_name} completato - RMSE: {metrics['RMSE']:.4f}\")\n            \n            return result\n            \n        except Exception as e:\n            print(f\"‚ùå {algorithm_name} fallito: {str(e)}\")\n            return None\n    \n    def _get_predictions(self, model, n_steps: int, model_info: Dict) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n        \"\"\"Estrai predizioni da diversi tipi di modello.\"\"\"\n        predictions = None\n        confidence_intervals = None\n        \n        model_type = model_info.get('type', 'unknown')\n        \n        if model_type in ['arima', 'sarima', 'exponential_smoothing']:\n            # Modelli statistici\n            if hasattr(model, 'forecast'):\n                if model_type in ['arima', 'sarima']:\n                    forecast_result = model.forecast(steps=n_steps, return_conf_int=True)\n                    if isinstance(forecast_result, tuple):\n                        predictions = forecast_result[0].values\n                        confidence_intervals = forecast_result[1].values\n                    else:\n                        predictions = forecast_result.values\n                else:\n                    predictions = model.forecast(steps=n_steps)\n            elif hasattr(model, 'predict'):\n                predictions = model.predict(start=len(model.model.endog), \n                                          end=len(model.model.endog) + n_steps - 1)\n        \n        elif model_type in ['sklearn', 'xgboost']:\n            # Modelli ML - necessario generare feature\n            X_test = model_info.get('X_test')\n            if X_test is not None:\n                predictions = model.predict(X_test)\n        \n        return predictions, confidence_intervals\n    \n    def _extract_feature_importance(self, model, model_info: Dict) -> Optional[Dict[str, float]]:\n        \"\"\"Estrai importanza feature se disponibile.\"\"\"\n        if hasattr(model, 'feature_importances_'):\n            feature_names = model_info.get('feature_names', [f'feature_{i}' for i in range(len(model.feature_importances_))])\n            return dict(zip(feature_names, model.feature_importances_))\n        return None\n\n# Inizializza sistema benchmark\nbenchmark = AlgorithmBenchmark(random_seed=RANDOM_SEED)\nprint(\"üèóÔ∏è  Infrastruttura di ricerca pronta!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üî¨ Implementazioni Algoritmi\n\nDefinisce implementazioni standardizzate di vari algoritmi di forecasting per un confronto equo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_lagged_features(series: pd.Series, n_lags: int = 12, \n                          include_stats: bool = True) -> pd.DataFrame:\n    \"\"\"Crea feature con ritardi per modelli ML.\"\"\"\n    features = pd.DataFrame(index=series.index)\n    \n    # Valori ritardati\n    for lag in range(1, n_lags + 1):\n        features[f'lag_{lag}'] = series.shift(lag)\n    \n    if include_stats:\n        # Statistiche rolling\n        for window in [3, 7, 30]:\n            if window <= len(series):\n                features[f'rolling_mean_{window}'] = series.rolling(window).mean().shift(1)\n                features[f'rolling_std_{window}'] = series.rolling(window).std().shift(1)\n        \n        # Feature basate sul tempo\n        if hasattr(series.index, 'dayofweek'):\n            features['dayofweek'] = series.index.dayofweek\n            features['month'] = series.index.month\n            features['quarter'] = series.index.quarter\n        \n        # Feature trend\n        features['trend'] = np.arange(len(series))\n    \n    return features.dropna()\n\n# Implementazioni algoritmi\ndef experiment_arima(data: pd.Series, order: Tuple[int, int, int] = (1, 1, 1)) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento modello ARIMA.\"\"\"\n    if PROJECT_IMPORTS_OK:\n        model = ARIMAForecaster(order=order)\n        model.fit(data)\n        return model, {'type': 'arima', 'order': order}\n    else:\n        model = ARIMA(data, order=order).fit()\n        return model, {'type': 'arima', 'order': order}\n\ndef experiment_sarima(data: pd.Series, order: Tuple[int, int, int] = (1, 1, 1),\n                     seasonal_order: Tuple[int, int, int, int] = (1, 1, 1, 12)) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento modello SARIMA.\"\"\"\n    if PROJECT_IMPORTS_OK:\n        model = SARIMAForecaster(order=order, seasonal_order=seasonal_order)\n        model.fit(data)\n        return model, {'type': 'sarima', 'order': order, 'seasonal_order': seasonal_order}\n    else:\n        model = SARIMAX(data, order=order, seasonal_order=seasonal_order).fit()\n        return model, {'type': 'sarima', 'order': order, 'seasonal_order': seasonal_order}\n\ndef experiment_exponential_smoothing(data: pd.Series, trend: str = 'add', \n                                   seasonal: str = 'add', seasonal_periods: int = 12) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento Exponential Smoothing.\"\"\"\n    model = ExponentialSmoothing(data, trend=trend, seasonal=seasonal, \n                               seasonal_periods=seasonal_periods).fit()\n    return model, {'type': 'exponential_smoothing', 'trend': trend, 'seasonal': seasonal}\n\ndef experiment_random_forest(data: pd.Series, n_estimators: int = 100, \n                           n_lags: int = 12, test_size: int = 12) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento Random Forest.\"\"\"\n    # Crea feature\n    features = create_lagged_features(data, n_lags=n_lags)\n    \n    # Prepara train/test\n    X = features.values[:-test_size]\n    y = data.iloc[n_lags:-test_size].values\n    X_test = features.values[-test_size:]\n    \n    # Addestra modello\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=RANDOM_SEED)\n    model.fit(X, y)\n    \n    return model, {\n        'type': 'sklearn', \n        'X_test': X_test,\n        'feature_names': features.columns.tolist()\n    }\n\ndef experiment_xgboost(data: pd.Series, n_estimators: int = 100, \n                      n_lags: int = 12, test_size: int = 12) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento XGBoost.\"\"\"\n    if not XGBOOST_AVAILABLE:\n        raise ImportError(\"XGBoost non disponibile\")\n    \n    # Crea feature\n    features = create_lagged_features(data, n_lags=n_lags)\n    \n    # Prepara train/test\n    X = features.values[:-test_size]\n    y = data.iloc[n_lags:-test_size].values\n    X_test = features.values[-test_size:]\n    \n    # Addestra modello\n    model = xgb.XGBRegressor(n_estimators=n_estimators, random_state=RANDOM_SEED)\n    model.fit(X, y)\n    \n    return model, {\n        'type': 'xgboost', \n        'X_test': X_test,\n        'feature_names': features.columns.tolist()\n    }\n\ndef experiment_svr(data: pd.Series, kernel: str = 'rbf', n_lags: int = 12, \n                  test_size: int = 12) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento Support Vector Regression.\"\"\"\n    # Crea feature\n    features = create_lagged_features(data, n_lags=n_lags)\n    \n    # Prepara train/test\n    X = features.values[:-test_size]\n    y = data.iloc[n_lags:-test_size].values\n    X_test = features.values[-test_size:]\n    \n    # Scala feature per SVR\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Addestra modello\n    model = SVR(kernel=kernel)\n    model.fit(X_scaled, y)\n    \n    # Wrappa modello con scaler\n    class ScaledSVR:\n        def __init__(self, model, scaler):\n            self.model = model\n            self.scaler = scaler\n        \n        def predict(self, X):\n            return self.model.predict(self.scaler.transform(X))\n    \n    wrapped_model = ScaledSVR(model, scaler)\n    \n    return wrapped_model, {\n        'type': 'sklearn',\n        'X_test': X_test,  # Usa non scalati per consistenza\n        'feature_names': features.columns.tolist()\n    }\n\nprint(\"üß™ Implementazioni algoritmi pronte!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìä Generazione e Preparazione Dataset\n\nCrea dataset sintetici diversi con caratteristiche differenti per testare la robustezza degli algoritmi."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_synthetic_ts(name: str, n_periods: int = 200, freq: str = 'D', \n                         trend_type: str = 'linear', seasonality: bool = True,\n                         noise_level: float = 0.1, anomalies: bool = False) -> pd.Series:\n    \"\"\"Genera serie temporali sintetiche con caratteristiche specificate.\"\"\"\n    \n    dates = pd.date_range('2020-01-01', periods=n_periods, freq=freq)\n    t = np.arange(n_periods)\n    \n    # Trend di base\n    if trend_type == 'linear':\n        trend = 0.1 * t + 100\n    elif trend_type == 'exponential':\n        trend = 100 * np.exp(0.001 * t)\n    elif trend_type == 'polynomial':\n        trend = 100 + 0.1 * t + 0.0001 * t**2\n    elif trend_type == 'none':\n        trend = np.full(n_periods, 100)\n    else:\n        trend = np.full(n_periods, 100)\n    \n    # Stagionalit√†\n    seasonal = np.zeros(n_periods)\n    if seasonality:\n        # Stagionalit√† annuale\n        seasonal += 10 * np.sin(2 * np.pi * t / 365.25)\n        # Stagionalit√† settimanale (se frequenza giornaliera)\n        if freq == 'D':\n            seasonal += 5 * np.sin(2 * np.pi * t / 7)\n        # Stagionalit√† mensile (se frequenza maggiore)\n        if freq in ['D', 'H']:\n            seasonal += 3 * np.sin(2 * np.pi * t / 30.44)\n    \n    # Rumore\n    noise = np.random.normal(0, noise_level * np.mean(trend), n_periods)\n    \n    # Combina componenti\n    series = trend + seasonal + noise\n    \n    # Aggiungi anomalie\n    if anomalies:\n        n_anomalies = max(1, n_periods // 50)  # ~2% anomalie\n        anomaly_idx = np.random.choice(n_periods, n_anomalies, replace=False)\n        for idx in anomaly_idx:\n            series[idx] *= np.random.choice([0.5, 1.5, 2.0])  # Moltiplicatore anomalia\n    \n    return pd.Series(series, index=dates, name=name)\n\n# Genera dataset di ricerca\nprint(\"üìä Generazione dataset di ricerca...\")\n\n# Dataset 1: Trend lineare semplice con stagionalit√†\nts_linear = generate_synthetic_ts(\n    'linear_seasonal', \n    n_periods=300, \n    trend_type='linear',\n    seasonality=True,\n    noise_level=0.05\n)\nbenchmark.add_dataset('Linear_Stagionale', ts_linear, 'Trend lineare con stagionalit√† annuale/settimanale')\n\n# Dataset 2: Crescita esponenziale\nts_exponential = generate_synthetic_ts(\n    'exponential_growth',\n    n_periods=200,\n    trend_type='exponential',\n    seasonality=False,\n    noise_level=0.1\n)\nbenchmark.add_dataset('Crescita_Esponenziale', ts_exponential, 'Pattern di crescita esponenziale')\n\n# Dataset 3: Rumore elevato con anomalie\nts_noisy = generate_synthetic_ts(\n    'noisy_anomalies',\n    n_periods=250,\n    trend_type='linear',\n    seasonality=True,\n    noise_level=0.3,\n    anomalies=True\n)\nbenchmark.add_dataset('Rumoroso_Anomalie', ts_noisy, 'Rumore elevato con anomalie')\n\n# Dataset 4: Serie stazionaria (senza trend)\nts_stationary = generate_synthetic_ts(\n    'stationary',\n    n_periods=200,\n    trend_type='none',\n    seasonality=True,\n    noise_level=0.1\n)\nbenchmark.add_dataset('Stazionaria', ts_stationary, 'Stazionaria con stagionalit√†')\n\n# Dataset 5: Trend polinomiale complesso\nts_polynomial = generate_synthetic_ts(\n    'polynomial',\n    n_periods=300,\n    trend_type='polynomial',\n    seasonality=True,\n    noise_level=0.08\n)\nbenchmark.add_dataset('Polinomiale', ts_polynomial, 'Trend polinomiale con stagionalit√†')\n\n# Visualizza dataset\nfig, axes = plt.subplots(3, 2, figsize=(15, 12))\naxes = axes.ravel()\n\nfor i, (name, dataset) in enumerate(benchmark.datasets.items()):\n    if i < 5:\n        ax = axes[i]\n        data = dataset['data']\n        ax.plot(data.index, data.values, linewidth=1.5, alpha=0.8)\n        ax.set_title(f'{name}\\n{dataset[\"description\"]}', fontsize=10)\n        ax.grid(True, alpha=0.3)\n        ax.tick_params(axis='x', rotation=45)\n\n# Rimuovi subplot vuoto\naxes[5].remove()\n\nplt.tight_layout()\nplt.savefig(plots_dir / 'dataset_ricerca.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"üìä Generati {len(benchmark.datasets)} dataset di ricerca\")\nfor name, dataset in benchmark.datasets.items():\n    print(f\"  ‚Ä¢ {name}: {dataset['length']} osservazioni - {dataset['description']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üß™ Esperimenti Algoritmi Completi\n\nEsegue esperimenti sistematici attraverso tutti gli algoritmi e dataset con valutazione rigorosa."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configurazione esperimenti\nTEST_SIZE = 30  # Numero di periodi per test out-of-sample\nexperiments_config = [\n    ('ARIMA_111', experiment_arima, {'order': (1, 1, 1)}),\n    ('ARIMA_212', experiment_arima, {'order': (2, 1, 2)}),\n    ('SARIMA_111_1111_12', experiment_sarima, {\n        'order': (1, 1, 1), \n        'seasonal_order': (1, 1, 1, 12)\n    }),\n    ('ExpSmoothing_Add', experiment_exponential_smoothing, {\n        'trend': 'add', \n        'seasonal': 'add', \n        'seasonal_periods': 12\n    }),\n    ('RandomForest_100', experiment_random_forest, {\n        'n_estimators': 100, \n        'n_lags': 12, \n        'test_size': TEST_SIZE\n    }),\n    ('RandomForest_500', experiment_random_forest, {\n        'n_estimators': 500, \n        'n_lags': 20, \n        'test_size': TEST_SIZE\n    }),\n]\n\n# Aggiungi XGBoost se disponibile\nif XGBOOST_AVAILABLE:\n    experiments_config.extend([\n        ('XGBoost_100', experiment_xgboost, {\n            'n_estimators': 100, \n            'n_lags': 12, \n            'test_size': TEST_SIZE\n        }),\n        ('XGBoost_300', experiment_xgboost, {\n            'n_estimators': 300, \n            'n_lags': 20, \n            'test_size': TEST_SIZE\n        }),\n    ])\n\n# Aggiungi SVR se disponibile\nif SVR_AVAILABLE:\n    experiments_config.extend([\n        ('SVR_RBF', experiment_svr, {\n            'kernel': 'rbf', \n            'n_lags': 12, \n            'test_size': TEST_SIZE\n        }),\n        ('SVR_Linear', experiment_svr, {\n            'kernel': 'linear', \n            'n_lags': 12, \n            'test_size': TEST_SIZE\n        }),\n    ])\n\nprint(f\"üî¨ Esecuzione di {len(experiments_config)} algoritmi su {len(benchmark.datasets)} dataset\")\nprint(f\"üìä Esperimenti totali: {len(experiments_config) * len(benchmark.datasets)}\")\n\n# Esegui tutti gli esperimenti\nresults_matrix = []\nexperiment_counter = 0\ntotal_experiments = len(experiments_config) * len(benchmark.datasets)\n\nfor dataset_name, dataset_info in benchmark.datasets.items():\n    print(f\"\\nüìä Dataset: {dataset_name}\")\n    print(\"=\" * 50)\n    \n    data = dataset_info['data']\n    train_data = data[:-TEST_SIZE]\n    test_data = data[-TEST_SIZE:]\n    \n    dataset_results = []\n    \n    for algo_name, algo_func, algo_params in experiments_config:\n        experiment_counter += 1\n        print(f\"\\n[{experiment_counter}/{total_experiments}] {algo_name} su {dataset_name}\")\n        \n        try:\n            result = benchmark.run_experiment(\n                algo_func, algo_name, train_data, test_data, **algo_params\n            )\n            \n            if result:\n                dataset_results.append(result)\n                \n                # Aggiungi alla matrice risultati\n                results_matrix.append({\n                    'Dataset': dataset_name,\n                    'Algoritmo': algo_name,\n                    'RMSE': result.metrics['RMSE'],\n                    'MAE': result.metrics['MAE'],\n                    'MAPE': result.metrics['MAPE'],\n                    'R2': result.metrics['R2'],\n                    'Tempo_Training': result.training_time,\n                    'Tempo_Predizione': result.prediction_time,\n                    'Memoria_MB': result.memory_usage\n                })\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Esperimento fallito: {e}\")\n            continue\n\nprint(f\"\\n‚úÖ Completati {len(results_matrix)} esperimenti con successo\")\n\n# Crea DataFrame risultati\nresults_df = pd.DataFrame(results_matrix)\nprint(f\"üìä Matrice risultati shape: {results_df.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìà Analisi Performance e Visualizzazione\n\nAnalisi completa dei risultati sperimentali con visualizzazioni professionali."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Statistiche Riassuntive Performance\nprint(\"üìä RIASSUNTO RISULTATI ESPERIMENTI\")\nprint(\"=\" * 60)\n\nif len(results_df) > 0:\n    # Classifica performance generale\n    print(\"\\nüèÜ Classifica Algoritmi Generale (per RMSE Medio):\")\n    avg_performance = results_df.groupby('Algoritmo').agg({\n        'RMSE': ['mean', 'std', 'min', 'max'],\n        'MAE': ['mean', 'std'],\n        'R2': ['mean', 'std'],\n        'Tempo_Training': ['mean', 'std'],\n        'Memoria_MB': ['mean', 'std']\n    }).round(4)\n    \n    avg_performance.columns = ['_'.join(col).strip() for col in avg_performance.columns]\n    ranking = avg_performance.sort_values('RMSE_mean')\n    \n    for i, (algo, row) in enumerate(ranking.iterrows(), 1):\n        print(f\"{i:2d}. {algo:20s} - RMSE: {row['RMSE_mean']:8.4f} (¬±{row['RMSE_std']:6.4f}) | \"\n              f\"R¬≤: {row['R2_mean']:6.3f} | Tempo: {row['Tempo_Training_mean']:6.2f}s\")\n    \n    # Analisi difficolt√† dataset\n    print(\"\\nüìä Classifica Difficolt√† Dataset (per RMSE Medio):\")\n    dataset_difficulty = results_df.groupby('Dataset')['RMSE'].agg(['mean', 'std']).round(4)\n    dataset_difficulty = dataset_difficulty.sort_values('mean', ascending=False)\n    \n    for i, (dataset, row) in enumerate(dataset_difficulty.iterrows(), 1):\n        print(f\"{i}. {dataset:20s} - RMSE Medio: {row['mean']:8.4f} (¬±{row['std']:6.4f})\")\n    \n    # Miglior algoritmo per dataset\n    print(\"\\nüéØ Miglior Algoritmo per Dataset:\")\n    best_per_dataset = results_df.loc[results_df.groupby('Dataset')['RMSE'].idxmin()]\n    \n    for _, row in best_per_dataset.iterrows():\n        print(f\"{row['Dataset']:20s} ‚Üí {row['Algoritmo']:20s} (RMSE: {row['RMSE']:7.4f})\")\n    \n    # Analisi Performance vs Efficienza\n    print(\"\\n‚ö° Analisi Performance vs Efficienza:\")\n    efficiency_df = results_df.groupby('Algoritmo').agg({\n        'RMSE': 'mean',\n        'Tempo_Training': 'mean',\n        'Memoria_MB': 'mean'\n    }).round(4)\n    \n    # Normalizza metriche per confronto (pi√π basso √® meglio per tutti)\n    efficiency_df['RMSE_norm'] = (efficiency_df['RMSE'] - efficiency_df['RMSE'].min()) / (efficiency_df['RMSE'].max() - efficiency_df['RMSE'].min())\n    efficiency_df['Tempo_norm'] = (efficiency_df['Tempo_Training'] - efficiency_df['Tempo_Training'].min()) / (efficiency_df['Tempo_Training'].max() - efficiency_df['Tempo_Training'].min())\n    efficiency_df['Memoria_norm'] = (efficiency_df['Memoria_MB'] - efficiency_df['Memoria_MB'].min()) / (efficiency_df['Memoria_MB'].max() - efficiency_df['Memoria_MB'].min())\n    \n    # Punteggio efficienza combinato (pi√π basso √® meglio)\n    efficiency_df['Punteggio_Efficienza'] = (efficiency_df['RMSE_norm'] + efficiency_df['Tempo_norm'] + efficiency_df['Memoria_norm']) / 3\n    efficiency_ranking = efficiency_df.sort_values('Punteggio_Efficienza')\n    \n    for i, (algo, row) in enumerate(efficiency_ranking.iterrows(), 1):\n        print(f\"{i:2d}. {algo:20s} - Punteggio: {row['Punteggio_Efficienza']:5.3f} | \"\n              f\"RMSE: {row['RMSE']:7.4f} | Tempo: {row['Tempo_Training']:6.2f}s | Memoria: {row['Memoria_MB']:5.1f}MB\")\n\nelse:\n    print(\"‚ùå Nessun esperimento riuscito da analizzare\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizzazioni Avanzate\nif len(results_df) > 0:\n    \n    # Crea dashboard di visualizzazione completo\n    fig = plt.figure(figsize=(20, 16))\n    \n    # 1. Mappa di Calore Performance\n    ax1 = plt.subplot(3, 3, 1)\n    pivot_rmse = results_df.pivot(index='Algoritmo', columns='Dataset', values='RMSE')\n    sns.heatmap(pivot_rmse, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax1)\n    ax1.set_title('Mappa di Calore Performance RMSE\\n(Pi√π Basso √® Meglio)', fontweight='bold')\n    ax1.set_xlabel('Dataset')\n    ax1.set_ylabel('Algoritmo')\n    \n    # 2. Distribuzione Performance Algoritmi\n    ax2 = plt.subplot(3, 3, 2)\n    results_df.boxplot(column='RMSE', by='Algoritmo', ax=ax2)\n    ax2.set_title('Distribuzione RMSE per Algoritmo')\n    ax2.set_xlabel('Algoritmo')\n    ax2.set_ylabel('RMSE')\n    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n    plt.suptitle('')  # Rimuovi titolo automatico\n    \n    # 3. Tempo Training vs Performance\n    ax3 = plt.subplot(3, 3, 3)\n    scatter = ax3.scatter(results_df['Tempo_Training'], results_df['RMSE'], \n                         c=results_df['R2'], s=50, alpha=0.7, cmap='viridis')\n    ax3.set_xlabel('Tempo Training (secondi)')\n    ax3.set_ylabel('RMSE')\n    ax3.set_title('Performance vs Tempo Training\\n(Colore: R¬≤)')\n    plt.colorbar(scatter, ax=ax3, label='R¬≤')\n    \n    # 4. Analisi Utilizzo Memoria\n    ax4 = plt.subplot(3, 3, 4)\n    memory_by_algo = results_df.groupby('Algoritmo')['Memoria_MB'].mean().sort_values()\n    bars = ax4.bar(range(len(memory_by_algo)), memory_by_algo.values)\n    ax4.set_xticks(range(len(memory_by_algo)))\n    ax4.set_xticklabels(memory_by_algo.index, rotation=45, ha='right')\n    ax4.set_ylabel('Utilizzo Memoria (MB)')\n    ax4.set_title('Utilizzo Memoria Medio per Algoritmo')\n    \n    # Aggiungi etichette valori sulle barre\n    for i, (bar, val) in enumerate(zip(bars, memory_by_algo.values)):\n        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n                f'{val:.1f}', ha='center', va='bottom', fontsize=8)\n    \n    # 5. Analisi Difficolt√† Dataset\n    ax5 = plt.subplot(3, 3, 5)\n    dataset_stats = results_df.groupby('Dataset')['RMSE'].agg(['mean', 'std'])\n    ax5.errorbar(range(len(dataset_stats)), dataset_stats['mean'], \n                yerr=dataset_stats['std'], fmt='o-', capsize=5, markersize=8)\n    ax5.set_xticks(range(len(dataset_stats)))\n    ax5.set_xticklabels(dataset_stats.index, rotation=45, ha='right')\n    ax5.set_ylabel('RMSE')\n    ax5.set_title('Difficolt√† Dataset\\n(RMSE Medio ¬± Std)')\n    ax5.grid(True, alpha=0.3)\n    \n    # 6. Confronto Performance R¬≤\n    ax6 = plt.subplot(3, 3, 6)\n    r2_by_algo = results_df.groupby('Algoritmo')['R2'].mean().sort_values(ascending=False)\n    bars = ax6.bar(range(len(r2_by_algo)), r2_by_algo.values, color='lightcoral')\n    ax6.set_xticks(range(len(r2_by_algo)))\n    ax6.set_xticklabels(r2_by_algo.index, rotation=45, ha='right')\n    ax6.set_ylabel('Punteggio R¬≤')\n    ax6.set_title('Punteggio R¬≤ Medio per Algoritmo')\n    ax6.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n    \n    # 7. Grafico Radar Multi-Metrica (per i top 5 algoritmi)\n    ax7 = plt.subplot(3, 3, 7, projection='polar')\n    \n    # Normalizza metriche per grafico radar\n    top_5_algos = results_df.groupby('Algoritmo')['RMSE'].mean().nsmallest(5).index\n    radar_data = results_df[results_df['Algoritmo'].isin(top_5_algos)].groupby('Algoritmo').agg({\n        'RMSE': 'mean',\n        'MAE': 'mean', \n        'R2': 'mean',\n        'Tempo_Training': 'mean',\n        'Memoria_MB': 'mean'\n    })\n    \n    # Normalizza (inverti per RMSE, MAE, Tempo, Memoria cos√¨ pi√π alto √® meglio)\n    radar_normalized = radar_data.copy()\n    for col in ['RMSE', 'MAE', 'Tempo_Training', 'Memoria_MB']:\n        radar_normalized[col] = 1 - (radar_data[col] - radar_data[col].min()) / (radar_data[col].max() - radar_data[col].min())\n    \n    # R2 √® gi√† pi√π-alto-√®-meglio\n    radar_normalized['R2'] = (radar_data['R2'] - radar_data['R2'].min()) / (radar_data['R2'].max() - radar_data['R2'].min())\n    \n    categories = ['RMSE', 'MAE', 'R¬≤', 'Velocit√†', 'Memoria']\n    N = len(categories)\n    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n    angles += angles[:1]  # Completa il cerchio\n    \n    colors = plt.cm.Set3(np.linspace(0, 1, len(top_5_algos)))\n    \n    for i, (algo, row) in enumerate(radar_normalized.iterrows()):\n        values = row.tolist()\n        values += values[:1]  # Completa il cerchio\n        ax7.plot(angles, values, 'o-', linewidth=2, label=algo, color=colors[i])\n        ax7.fill(angles, values, alpha=0.25, color=colors[i])\n    \n    ax7.set_xticks(angles[:-1])\n    ax7.set_xticklabels(categories)\n    ax7.set_ylim(0, 1)\n    ax7.set_title('Performance Multi-Metrica\\n(Top 5 Algoritmi)', y=1.08)\n    ax7.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n    \n    # 8. Scatter Accuratezza Predizione\n    ax8 = plt.subplot(3, 3, 8)\n    for algo in results_df['Algoritmo'].unique():\n        algo_data = results_df[results_df['Algoritmo'] == algo]\n        ax8.scatter(algo_data['MAE'], algo_data['MAPE'], label=algo, alpha=0.7, s=50)\n    \n    ax8.set_xlabel('MAE')\n    ax8.set_ylabel('MAPE (%)')\n    ax8.set_title('MAE vs MAPE per Algoritmo')\n    ax8.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    ax8.grid(True, alpha=0.3)\n    \n    # 9. Distribuzione Tempo Training\n    ax9 = plt.subplot(3, 3, 9)\n    results_df['Tempo_Training'].hist(bins=20, alpha=0.7, ax=ax9)\n    ax9.axvline(results_df['Tempo_Training'].mean(), color='red', linestyle='--', \n                label=f'Media: {results_df[\"Tempo_Training\"].mean():.2f}s')\n    ax9.set_xlabel('Tempo Training (secondi)')\n    ax9.set_ylabel('Frequenza')\n    ax9.set_title('Distribuzione Tempo Training')\n    ax9.legend()\n    ax9.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(plots_dir / 'analisi_completa.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"üìä Visualizzazione analisi completa salvata in {plots_dir / 'analisi_completa.png'}\")\n    \nelse:\n    print(\"‚ùå Nessun risultato da visualizzare\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üîç Analisi Approfondita: Importanza Feature e Interpretabilit√† Modelli\n\nAnalizza quali feature e pattern i diversi algoritmi stanno apprendendo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analisi Importanza Feature per Modelli Basati su Albero\nprint(\"üîç ANALISI IMPORTANZA FEATURE\")\nprint(\"=\" * 50)\n\n# Raccogli importanza feature dagli esperimenti riusciti\nfeature_importance_data = []\n\nfor result in benchmark.results:\n    if result.feature_importance is not None:\n        for feature, importance in result.feature_importance.items():\n            feature_importance_data.append({\n                'Algoritmo': result.algorithm_name,\n                'Feature': feature,\n                'Importanza': importance\n            })\n\nif feature_importance_data:\n    fi_df = pd.DataFrame(feature_importance_data)\n    \n    # Importanza feature media per algoritmo\n    avg_importance = fi_df.groupby(['Algoritmo', 'Feature'])['Importanza'].mean().reset_index()\n    \n    # Visualizza importanza feature\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # Top feature complessive\n    ax1 = axes[0, 0]\n    top_features = fi_df.groupby('Feature')['Importanza'].mean().sort_values(ascending=False).head(15)\n    bars = ax1.bar(range(len(top_features)), top_features.values)\n    ax1.set_xticks(range(len(top_features)))\n    ax1.set_xticklabels(top_features.index, rotation=45, ha='right')\n    ax1.set_ylabel('Importanza Media')\n    ax1.set_title('Top 15 Feature Pi√π Importanti (Complessivo)')\n    ax1.grid(True, alpha=0.3)\n    \n    # Importanza feature per algoritmo\n    ax2 = axes[0, 1]\n    algorithms_with_fi = avg_importance['Algoritmo'].unique()\n    if len(algorithms_with_fi) > 0:\n        pivot_fi = avg_importance.pivot(index='Feature', columns='Algoritmo', values='Importanza')\n        # Seleziona top 10 feature per chiarezza\n        top_10_features = pivot_fi.sum(axis=1).sort_values(ascending=False).head(10).index\n        pivot_fi_top = pivot_fi.loc[top_10_features]\n        \n        sns.heatmap(pivot_fi_top, annot=True, fmt='.3f', cmap='Blues', ax=ax2)\n        ax2.set_title('Importanza Feature per Algoritmo\\n(Top 10 Feature)')\n        ax2.set_xlabel('Algoritmo')\n        ax2.set_ylabel('Feature')\n    \n    # Analisi tipo feature\n    ax3 = axes[1, 0]\n    \n    def categorize_feature(feature_name):\n        if 'lag_' in feature_name:\n            return 'Valori Ritardati'\n        elif 'rolling_mean' in feature_name:\n            return 'Media Mobile'\n        elif 'rolling_std' in feature_name:\n            return 'Deviazione Std Mobile'\n        elif feature_name in ['dayofweek', 'month', 'quarter']:\n            return 'Feature Temporali'\n        elif feature_name == 'trend':\n            return 'Trend'\n        else:\n            return 'Altro'\n    \n    fi_df['Tipo_Feature'] = fi_df['Feature'].apply(categorize_feature)\n    feature_type_importance = fi_df.groupby('Tipo_Feature')['Importanza'].mean().sort_values(ascending=False)\n    \n    colors = plt.cm.Set3(np.arange(len(feature_type_importance)))\n    wedges, texts, autotexts = ax3.pie(feature_type_importance.values, \n                                      labels=feature_type_importance.index,\n                                      autopct='%1.1f%%', \n                                      colors=colors,\n                                      startangle=90)\n    ax3.set_title('Importanza Feature per Tipo\\n(Media attraverso tutti gli algoritmi)')\n    \n    # Analisi importanza lag\n    ax4 = axes[1, 1]\n    lag_features = fi_df[fi_df['Feature'].str.contains('lag_')].copy()\n    if not lag_features.empty:\n        lag_features['Numero_Lag'] = lag_features['Feature'].str.extract(r'lag_(\\d+)').astype(int)\n        lag_importance = lag_features.groupby('Numero_Lag')['Importanza'].mean().sort_index()\n        \n        ax4.plot(lag_importance.index, lag_importance.values, 'o-', linewidth=2, markersize=6)\n        ax4.set_xlabel('Numero Lag')\n        ax4.set_ylabel('Importanza Media')\n        ax4.set_title('Importanza per Numero Lag')\n        ax4.grid(True, alpha=0.3)\n        \n        # Evidenzia lag pi√π importanti\n        top_3_lags = lag_importance.nlargest(3)\n        for lag, importance in top_3_lags.items():\n            ax4.annotate(f'Lag {lag}', (lag, importance), \n                        xytext=(5, 5), textcoords='offset points',\n                        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n    \n    plt.tight_layout()\n    plt.savefig(plots_dir / 'analisi_importanza_feature.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # Stampa insight principali\n    print(\"\\nüéØ INSIGHT CHIAVE:\")\n    print(f\"\\n1. Feature Pi√π Importanti (Top 5):\")\n    for i, (feature, importance) in enumerate(top_features.head().items(), 1):\n        print(f\"   {i}. {feature:20s}: {importance:.4f}\")\n    \n    print(f\"\\n2. Classifiche Tipo Feature:\")\n    for i, (ftype, importance) in enumerate(feature_type_importance.items(), 1):\n        print(f\"   {i}. {ftype:15s}: {importance:.4f}\")\n    \n    if not lag_features.empty:\n        print(f\"\\n3. Lag Pi√π Importanti:\")\n        for i, (lag, importance) in enumerate(top_3_lags.items(), 1):\n            print(f\"   {i}. Lag {lag:2d}        : {importance:.4f}\")\n\nelse:\n    print(\"‚ùå Nessun dato importanza feature disponibile\")\n    print(\"   Questo accade di solito quando vengono testati solo modelli statistici\")\n    print(\"   Prova ad eseguire modelli basati su albero (RandomForest, XGBoost) per analisi feature\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üöÄ Ricerca Avanzata: Metodi Ensemble e Approcci Ibridi\n\nSperimenta con la combinazione di diversi algoritmi per prestazioni migliorate."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üöÄ RICERCA AVANZATA: METODI ENSEMBLE\")\nprint(\"=\" * 50)\n\nif len(benchmark.results) >= 2:\n    \n    # Raggruppa risultati per dataset per creazione ensemble\n    ensemble_results = []\n    \n    for dataset_name in benchmark.datasets.keys():\n        print(f\"\\nüìä Creazione ensemble per {dataset_name}...\")\n        \n        # Ottieni tutti i risultati per questo dataset\n        dataset_results = []\n        for result in benchmark.results:\n            # Accoppia risultati al dataset (accoppiamento semplificato)\n            if result.predictions is not None and len(result.predictions) == TEST_SIZE:\n                dataset_results.append(result)\n        \n        if len(dataset_results) >= 2:\n            print(f\"   Trovati {len(dataset_results)} modelli per ensemble\")\n            \n            # Ottieni dati test per valutazione\n            test_data = benchmark.datasets[dataset_name]['data'][-TEST_SIZE:]\n            y_true = test_data.values\n            \n            # 1. Ensemble Media Semplice\n            predictions_matrix = np.column_stack([r.predictions for r in dataset_results])\n            ensemble_simple = np.mean(predictions_matrix, axis=1)\n            \n            # 2. Media Pesata (pesi RMSE inverso)\n            weights = []\n            for result in dataset_results:\n                rmse = result.metrics['RMSE']\n                weight = 1.0 / rmse if rmse > 0 else 1.0\n                weights.append(weight)\n            \n            weights = np.array(weights)\n            weights = weights / np.sum(weights)  # Normalizza\n            \n            ensemble_weighted = np.average(predictions_matrix, axis=1, weights=weights)\n            \n            # 3. Ensemble Migliori N Modelli (top 3)\n            best_n = min(3, len(dataset_results))\n            sorted_results = sorted(dataset_results, key=lambda x: x.metrics['RMSE'])\n            best_predictions = np.column_stack([r.predictions for r in sorted_results[:best_n]])\n            ensemble_best_n = np.mean(best_predictions, axis=1)\n            \n            # Valuta metodi ensemble\n            ensemble_methods = {\n                'Media_Semplice': ensemble_simple,\n                'Media_Pesata': ensemble_weighted,\n                f'Migliori_{best_n}_Media': ensemble_best_n\n            }\n            \n            for method_name, predictions in ensemble_methods.items():\n                metrics = benchmark.evaluate_model(y_true, predictions)\n                \n                ensemble_results.append({\n                    'Dataset': dataset_name,\n                    'Metodo': f'Ensemble_{method_name}',\n                    'RMSE': metrics['RMSE'],\n                    'MAE': metrics['MAE'],\n                    'MAPE': metrics['MAPE'],\n                    'R2': metrics['R2'],\n                    'Predizioni': predictions\n                })\n                \n                print(f\"   {method_name:20s}: RMSE = {metrics['RMSE']:.4f}, R¬≤ = {metrics['R2']:.4f}\")\n    \n    if ensemble_results:\n        ensemble_df = pd.DataFrame(ensemble_results)\n        \n        print(\"\\nüèÜ RIASSUNTO PERFORMANCE ENSEMBLE\")\n        print(\"=\" * 40)\n        \n        # Confronta ensemble vs modelli individuali\n        print(\"\\nüìä Ensemble vs Miglior Modello Individuale:\")\n        \n        for dataset_name in ensemble_df['Dataset'].unique():\n            print(f\"\\n{dataset_name}:\")\n            \n            # Miglior modello individuale per questo dataset\n            individual_results = [r for r in benchmark.results \n                                if len(r.predictions) == TEST_SIZE and r.predictions is not None]\n            if individual_results:\n                best_individual = min(individual_results, key=lambda x: x.metrics['RMSE'])\n                print(f\"   Miglior Individuale  : {best_individual.algorithm_name:20s} RMSE = {best_individual.metrics['RMSE']:.4f}\")\n            \n            # Miglior ensemble per questo dataset\n            dataset_ensembles = ensemble_df[ensemble_df['Dataset'] == dataset_name]\n            best_ensemble = dataset_ensembles.loc[dataset_ensembles['RMSE'].idxmin()]\n            print(f\"   Miglior Ensemble     : {best_ensemble['Metodo']:20s} RMSE = {best_ensemble['RMSE']:.4f}\")\n            \n            if individual_results:\n                improvement = (best_individual.metrics['RMSE'] - best_ensemble['RMSE']) / best_individual.metrics['RMSE'] * 100\n                print(f\"   Miglioramento        : {improvement:+6.2f}%\")\n        \n        # Visualizza performance ensemble\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        \n        # Confronto metodi ensemble\n        ax1 = axes[0, 0]\n        ensemble_perf = ensemble_df.groupby('Metodo')['RMSE'].mean().sort_values()\n        bars = ax1.bar(range(len(ensemble_perf)), ensemble_perf.values)\n        ax1.set_xticks(range(len(ensemble_perf)))\n        ax1.set_xticklabels([m.replace('Ensemble_', '') for m in ensemble_perf.index], rotation=45)\n        ax1.set_ylabel('RMSE Medio')\n        ax1.set_title('Confronto Metodi Ensemble')\n        ax1.grid(True, alpha=0.3)\n        \n        # Scatter Ensemble vs Individuale\n        ax2 = axes[0, 1]\n        \n        # Raccogli performance modelli individuali\n        individual_rmse = []\n        ensemble_rmse = []\n        \n        for dataset_name in ensemble_df['Dataset'].unique():\n            # Miglior individuale\n            individual_results = [r for r in benchmark.results \n                                if len(r.predictions) == TEST_SIZE and r.predictions is not None]\n            if individual_results:\n                best_individual = min(individual_results, key=lambda x: x.metrics['RMSE'])\n                individual_rmse.append(best_individual.metrics['RMSE'])\n                \n                # Miglior ensemble\n                dataset_ensembles = ensemble_df[ensemble_df['Dataset'] == dataset_name]\n                best_ensemble_rmse = dataset_ensembles['RMSE'].min()\n                ensemble_rmse.append(best_ensemble_rmse)\n        \n        if individual_rmse and ensemble_rmse:\n            ax2.scatter(individual_rmse, ensemble_rmse, s=100, alpha=0.7)\n            \n            # Aggiungi linea diagonale (y=x)\n            min_val = min(min(individual_rmse), min(ensemble_rmse))\n            max_val = max(max(individual_rmse), max(ensemble_rmse))\n            ax2.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='Performance Uguale')\n            \n            ax2.set_xlabel('RMSE Miglior Individuale')\n            ax2.set_ylabel('RMSE Miglior Ensemble')\n            ax2.set_title('Performance Ensemble vs Individuale\\n(Sotto linea = Ensemble Migliore)')\n            ax2.legend()\n            ax2.grid(True, alpha=0.3)\n        \n        # Confronto predizioni per un dataset\n        ax3 = axes[1, 0]\n        if len(ensemble_results) > 0:\n            sample_result = ensemble_results[0]\n            sample_dataset = sample_result['Dataset']\n            test_data = benchmark.datasets[sample_dataset]['data'][-TEST_SIZE:]\n            \n            ax3.plot(test_data.index, test_data.values, 'k-', linewidth=2, label='Reale', alpha=0.8)\n            \n            # Plotta predizioni ensemble\n            dataset_ensembles = [r for r in ensemble_results if r['Dataset'] == sample_dataset]\n            colors = plt.cm.Set1(np.linspace(0, 1, len(dataset_ensembles)))\n            \n            for i, result in enumerate(dataset_ensembles):\n                ax3.plot(test_data.index, result['Predizioni'], '--', \n                        color=colors[i], label=result['Metodo'].replace('Ensemble_', ''), alpha=0.7)\n            \n            ax3.set_title(f'Confronto Predizioni: {sample_dataset}')\n            ax3.set_ylabel('Valore')\n            ax3.legend()\n            ax3.grid(True, alpha=0.3)\n        \n        # Distribuzione pesi per ensemble pesato\n        ax4 = axes[1, 1]\n        if len(dataset_results) > 0:\n            algo_names = [r.algorithm_name for r in dataset_results]\n            bars = ax4.bar(range(len(weights)), weights)\n            ax4.set_xticks(range(len(weights)))\n            ax4.set_xticklabels(algo_names, rotation=45, ha='right')\n            ax4.set_ylabel('Peso')\n            ax4.set_title('Pesi Ensemble\\n(Basato su RMSE Inverso)')\n            ax4.grid(True, alpha=0.3)\n            \n            # Aggiungi valori pesi sulle barre\n            for i, (bar, weight) in enumerate(zip(bars, weights)):\n                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n                        f'{weight:.3f}', ha='center', va='bottom', fontsize=9)\n        \n        plt.tight_layout()\n        plt.savefig(plots_dir / 'analisi_ensemble.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        \n    else:\n        print(\"‚ùå Nessun risultato ensemble generato\")\n        \nelse:\n    print(\"‚ùå Servono almeno 2 esperimenti riusciti per metodi ensemble\")\n    print(\"   Esegui prima pi√π esperimenti algoritmici\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìã Riassunto Ricerca ed Export\n\nRiassume tutti i risultati ed esporta i risultati per ulteriori analisi o reportistica."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üìã RIASSUNTO RICERCA ED EXPORT\")\nprint(\"=\" * 50)\n\n# Crea riassunto ricerca completo\nresearch_summary = {\n    'info_esperimento': {\n        'timestamp': pd.Timestamp.now().isoformat(),\n        'esperimenti_totali': len(benchmark.results),\n        'dataset_testati': len(benchmark.datasets),\n        'algoritmi_testati': len(set(r.algorithm_name for r in benchmark.results)),\n        'dimensione_test': TEST_SIZE,\n        'random_seed': RANDOM_SEED\n    },\n    'risultati_chiave': {},\n    'raccomandazioni': []\n}\n\nif len(results_df) > 0:\n    # Risultati chiave\n    best_overall = results_df.loc[results_df['RMSE'].idxmin()]\n    worst_overall = results_df.loc[results_df['RMSE'].idxmax()]\n    \n    research_summary['risultati_chiave'] = {\n        'miglior_algoritmo_complessivo': best_overall['Algoritmo'],\n        'miglior_performance_rmse': float(best_overall['RMSE']),\n        'peggior_algoritmo_complessivo': worst_overall['Algoritmo'],\n        'peggior_performance_rmse': float(worst_overall['RMSE']),\n        'range_performance': float(worst_overall['RMSE'] - best_overall['RMSE']),\n        'tempo_training_medio': float(results_df['Tempo_Training'].mean()),\n        'utilizzo_memoria_medio': float(results_df['Memoria_MB'].mean())\n    }\n    \n    # Classifiche algoritmi\n    algo_ranking = results_df.groupby('Algoritmo')['RMSE'].mean().sort_values()\n    research_summary['classifica_algoritmi'] = {\n        str(algo): float(rmse) for algo, rmse in algo_ranking.items()\n    }\n    \n    # Classifica difficolt√† dataset\n    dataset_ranking = results_df.groupby('Dataset')['RMSE'].mean().sort_values(ascending=False)\n    research_summary['difficolta_dataset'] = {\n        str(dataset): float(rmse) for dataset, rmse in dataset_ranking.items()\n    }\n    \n    # Genera raccomandazioni\n    recommendations = []\n    \n    # Raccomandazioni performance\n    top_3_algos = algo_ranking.head(3)\n    recommendations.append(f\"Per massima accuratezza, considera: {', '.join(top_3_algos.index)}\")\n    \n    # Raccomandazioni velocit√†\n    fast_algos = results_df[results_df['Tempo_Training'] < results_df['Tempo_Training'].median()]\n    if not fast_algos.empty:\n        best_fast = fast_algos.loc[fast_algos['RMSE'].idxmin()]['Algoritmo']\n        recommendations.append(f\"Per equilibrio velocit√†-accuratezza, considera: {best_fast}\")\n    \n    # Raccomandazioni memoria\n    low_memory = results_df[results_df['Memoria_MB'] < results_df['Memoria_MB'].median()]\n    if not low_memory.empty:\n        best_memory = low_memory.loc[low_memory['RMSE'].idxmin()]['Algoritmo']\n        recommendations.append(f\"Per efficienza memoria, considera: {best_memory}\")\n    \n    # Raccomandazioni specifiche dataset\n    hardest_dataset = dataset_ranking.index[0]\n    best_on_hardest = results_df[results_df['Dataset'] == hardest_dataset].loc[\n        results_df[results_df['Dataset'] == hardest_dataset]['RMSE'].idxmin()\n    ]['Algoritmo']\n    recommendations.append(f\"Per dataset difficili (come {hardest_dataset}), considera: {best_on_hardest}\")\n    \n    research_summary['raccomandazioni'] = recommendations\n\n# Salva risultati dettagliati\ntimestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n\n# 1. Salva DataFrame risultati\nif len(results_df) > 0:\n    results_file = output_dir / f'risultati_esperimenti_{timestamp}.csv'\n    results_df.to_csv(results_file, index=False)\n    print(f\"üìä Risultati dettagliati salvati in: {results_file}\")\n\n# 2. Salva riassunto ricerca\nsummary_file = output_dir / f'riassunto_ricerca_{timestamp}.json'\nwith open(summary_file, 'w') as f:\n    json.dump(research_summary, f, indent=2)\nprint(f\"üìã Riassunto ricerca salvato in: {summary_file}\")\n\n# 3. Salva oggetti modello (per ulteriori analisi)\nmodels_file = output_dir / f'modelli_addestrati_{timestamp}.pkl'\nwith open(models_file, 'wb') as f:\n    pickle.dump(benchmark.results, f)\nprint(f\"üî¨ Oggetti modello salvati in: {models_file}\")\n\n# 4. Crea report dettagliato\nreport_content = f\"\"\"# Report Ricerca Forecasting Serie Temporali\n\n**Generato:** {pd.Timestamp.now()}\n**Esperimenti:** {len(benchmark.results)}\n**Dataset:** {len(benchmark.datasets)}\n**Random Seed:** {RANDOM_SEED}\n\n## Sommario Esecutivo\n\n\"\"\"\n\nif len(results_df) > 0:\n    report_content += f\"\"\"\n### Risultati Chiave\n\n- **Miglior Algoritmo Complessivo:** {research_summary['risultati_chiave']['miglior_algoritmo_complessivo']} (RMSE: {research_summary['risultati_chiave']['miglior_performance_rmse']:.4f})\n- **Range Performance:** {research_summary['risultati_chiave']['range_performance']:.4f} unit√† RMSE\n- **Tempo Training Medio:** {research_summary['risultati_chiave']['tempo_training_medio']:.2f} secondi\n- **Utilizzo Memoria Medio:** {research_summary['risultati_chiave']['utilizzo_memoria_medio']:.1f} MB\n\n### Classifica Algoritmi (per RMSE)\n\n\"\"\"\n    for i, (algo, rmse) in enumerate(research_summary['classifica_algoritmi'].items(), 1):\n        report_content += f\"{i}. **{algo}**: {rmse:.4f}\\n\"\n    \n    report_content += \"\\n### Raccomandazioni\\n\\n\"\n    for rec in research_summary['raccomandazioni']:\n        report_content += f\"- {rec}\\n\"\n\nreport_content += f\"\"\"\n\n## Dataset Analizzati\n\n\"\"\"\n\nfor name, info in benchmark.datasets.items():\n    report_content += f\"- **{name}**: {info['length']} osservazioni - {info['description']}\\n\"\n\nreport_content += f\"\"\"\n\n## File Generati\n\n- `risultati_esperimenti_{timestamp}.csv`: Risultati sperimentali dettagliati\n- `riassunto_ricerca_{timestamp}.json`: Dati riassunto strutturati\n- `modelli_addestrati_{timestamp}.pkl`: Oggetti modello serializzati\n- `plots/`: File visualizzazioni\n\n---\n*Generato da Ambiente R&D ARIMA Forecaster*\n\"\"\"\n\nreport_file = output_dir / f'report_ricerca_{timestamp}.md'\nwith open(report_file, 'w') as f:\n    f.write(report_content)\nprint(f\"üìÑ Report ricerca salvato in: {report_file}\")\n\nprint(\"\\nüéâ SESSIONE DI RICERCA COMPLETATA!\")\nprint(\"=\" * 50)\nprint(f\"üìÅ Tutti gli output salvati in: {output_dir}\")\nprint(f\"üìä Esperimenti riusciti totali: {len(benchmark.results)}\")\nprint(f\"üéØ Algoritmo con migliori performance: {research_summary['risultati_chiave'].get('miglior_algoritmo_complessivo', 'N/A')}\")\n\nif len(results_df) > 0:\n    print(f\"üìà Range miglioramento performance: {research_summary['risultati_chiave']['range_performance']:.4f} unit√† RMSE\")\n\nprint(\"\\nüí° Prossimi Passi:\")\nprint(\"   1. Rivedi visualizzazioni e report generati\")\nprint(\"   2. Investiga ulteriormente algoritmi top-performing\")\nprint(\"   3. Considera metodi ensemble per produzione\")\nprint(\"   4. Testa su dataset aggiuntivi del mondo reale\")\nprint(\"   5. Implementa ottimizzazione iperparametri per migliori modelli\")\n\n# Mostra tabella riassunto finale\nif len(results_df) > 0:\n    print(\"\\nüìä RIASSUNTO PERFORMANCE FINALE:\")\n    summary_table = results_df.groupby('Algoritmo').agg({\n        'RMSE': ['mean', 'std'],\n        'R2': 'mean',\n        'Tempo_Training': 'mean'\n    }).round(4)\n    \n    summary_table.columns = ['RMSE_media', 'RMSE_std', 'R2_media', 'Tempo_media']\n    summary_table = summary_table.sort_values('RMSE_media')\n    \n    print(summary_table.to_string())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéØ Conclusioni Ricerca e Direzioni Future\n\nQuesto notebook fornisce un ambiente R&D completo per la ricerca sul forecasting di serie temporali. Le capacit√† chiave includono:\n\n### ‚úÖ **Funzionalit√† Implementate**\n\n1. **Confronto Sistematico Algoritmi**: Benchmarking equo attraverso algoritmi multipli\n2. **Valutazione Completa**: 10+ metriche di performance inclusi test statistici\n3. **Generazione Dataset Sintetici**: Esperimenti controllati con caratteristiche note\n4. **Analisi Importanza Feature**: Comprensione di cosa apprendono i modelli\n5. **Metodi Ensemble**: Combinazione modelli per performance migliorate\n6. **Visualizzazioni Professionali**: Grafici e analisi pronti per pubblicazione\n7. **Reporting Automatizzato**: Riassunti strutturati e raccomandazioni\n\n### üî¨ **Metodologia di Ricerca**\n\n- **Esperimenti Controllati**: Split train/test standardizzati e valutazione\n- **Tipi Dati Multipli**: Pattern lineari, esponenziali, rumorosi, stazionari\n- **Confronto Equo**: Stesso framework preprocessing e valutazione\n- **Rigore Statistico**: Metriche multiple e intervalli di confidenza\n- **Riproducibilit√†**: Random seed fissi e logging dettagliato\n\n### üöÄ **Miglioramenti Futuri**\n\n1. **Modelli Deep Learning**: Aggiungere architetture LSTM, GRU e Transformer\n2. **Ottimizzazione Iperparametri**: Tuning automatizzato con Optuna/Bayesiano\n3. **Cross-Validation**: Convalida incrociata serie temporali per valutazione robusta\n4. **Integrazione Dataset Reali**: Caricamento automatico dataset benchmark\n5. **Test Statistici**: Test Diebold-Mariano per significativit√†\n6. **Dashboard Interattiva**: Interfaccia Streamlit per sperimentazione real-time\n7. **Integrazione MLOps**: Versioning modelli e tracking esperimenti\n\n### üìà **Linee Guida Utilizzo**\n\n1. **Fase Ricerca**: Usa questo notebook per esplorare nuovi algoritmi e capire il loro comportamento\n2. **Fase Sviluppo**: Prototipa nuovi approcci e valida concetti\n3. **Benchmarking**: Confronta i tuoi algoritmi contro baseline consolidate\n4. **Pubblicazione**: Usa visualizzazioni e risultati generati in paper di ricerca\n\n---\n\n**Questo ambiente R&D consente ricerca sistematica, riproducibile e ricca di insight sul forecasting di serie temporali. Modificalo ed estendilo in base alle tue specifiche esigenze di ricerca!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}