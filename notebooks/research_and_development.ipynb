{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🧪 Ambiente R&D: Algoritmi Avanzati di Forecasting per Serie Temporali\n\n**Notebook di Ricerca e Sviluppo**  \n*Ambiente professionale per sperimentare algoritmi di forecasting all'avanguardia*\n\n---\n\n## 📋 Obiettivi di Ricerca\n\nQuesto notebook serve come ambiente R&D completo per:\n\n1. **🔬 Sperimentazione Algoritmica**: Testare e confrontare nuovi metodi di forecasting\n2. **📊 Benchmarking delle Performance**: Valutazione rigorosa attraverso metriche multiple\n3. **🎯 Sviluppo di Metodi**: Prototipare e validare approcci innovativi\n4. **📈 Ricerca su Modelli Ibridi**: Combinare diverse tecniche per prestazioni migliorate\n5. **🔍 Analisi Approfondita**: Capire quando e perché diversi metodi funzionano\n\n---\n\n## 🏗️ Framework di Ricerca\n\n### Algoritmi da Esplorare:\n- **Classici**: ARIMA, SARIMA, Exponential Smoothing\n- **Machine Learning**: Random Forest, XGBoost, SVR\n- **Deep Learning**: LSTM, GRU, modelli basati su Transformer\n- **Metodi Ensemble**: Media pesata, stacking, blending\n- **Approcci Ibridi**: Combinazioni Statistiche + ML\n\n### Framework di Valutazione:\n- **Metriche di Accuratezza**: MAE, RMSE, MAPE, SMAPE\n- **Test Statistici**: Diebold-Mariano, Model Confidence Set\n- **Robustezza**: Performance attraverso diverse caratteristiche dei dati\n- **Efficienza Computazionale**: Tempo di training, utilizzo memoria\n- **Interpretabilità**: Spiegabilità del modello e importanza delle feature"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🔧 Setup e Import\n\nInizializza l'ambiente di ricerca con tutte le librerie e configurazioni necessarie."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Librerie core\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pathlib import Path\nimport sys\nimport time\nfrom typing import Dict, List, Tuple, Any, Optional, Union\nfrom dataclasses import dataclass\nfrom collections import defaultdict\nimport pickle\nimport json\n\n# Statistiche e serie temporali\nfrom scipy import stats\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nimport statsmodels.api as sm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# Librerie ML avanzate (installa se necessario)\ntry:\n    import xgboost as xgb\n    XGBOOST_AVAILABLE = True\nexcept ImportError:\n    print(\"⚠️  XGBoost non disponibile. Installa con: pip install xgboost\")\n    XGBOOST_AVAILABLE = False\n\ntry:\n    from sklearn.svm import SVR\n    SVR_AVAILABLE = True\nexcept ImportError:\n    SVR_AVAILABLE = False\n\n# Aggiungi percorso progetto per import\nproject_root = Path().resolve()\nif 'notebooks' in str(project_root):\n    project_root = project_root.parent\nsys.path.insert(0, str(project_root / 'src'))\n\n# Import del progetto\ntry:\n    from arima_forecaster import ARIMAForecaster, SARIMAForecaster\n    from arima_forecaster.data import DataLoader, TimeSeriesPreprocessor\n    from arima_forecaster.evaluation import ModelEvaluator\n    from arima_forecaster.visualization import ForecastPlotter\n    from arima_forecaster.core import ARIMAModelSelector, SARIMAModelSelector\n    from arima_forecaster.utils import setup_logger\n    PROJECT_IMPORTS_OK = True\nexcept ImportError as e:\n    print(f\"⚠️  Import del progetto non disponibili: {e}\")\n    print(\"Esegui questo notebook dalla root del progetto o installa il package in modalità sviluppo\")\n    PROJECT_IMPORTS_OK = False\n\n# Configurazione\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\n\n# Configurazione ricerca\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\n# Crea directory di output\noutput_dir = project_root / \"outputs\" / \"research\"\noutput_dir.mkdir(parents=True, exist_ok=True)\nplots_dir = output_dir / \"plots\"\nplots_dir.mkdir(exist_ok=True)\nmodels_dir = output_dir / \"models\"\nmodels_dir.mkdir(exist_ok=True)\n\nprint(\"🔬 Ambiente R&D inizializzato con successo!\")\nprint(f\"📁 Directory output: {output_dir}\")\nprint(f\"🎯 Random seed: {RANDOM_SEED}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🏗️ Infrastruttura di Ricerca\n\nDefinisce le classi e funzioni core per la sperimentazione algoritmica sistematica."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass ExperimentResult:\n    \"\"\"Contenitore per i risultati degli esperimenti.\"\"\"\n    algorithm_name: str\n    parameters: Dict[str, Any]\n    training_time: float\n    prediction_time: float\n    memory_usage: float\n    metrics: Dict[str, float]\n    predictions: np.ndarray\n    model_object: Any = None\n    feature_importance: Optional[Dict[str, float]] = None\n    confidence_intervals: Optional[np.ndarray] = None\n    \nclass AlgorithmBenchmark:\n    \"\"\"Sistema di benchmarking professionale per algoritmi di serie temporali.\"\"\"\n    \n    def __init__(self, random_seed: int = 42):\n        self.random_seed = random_seed\n        self.results: List[ExperimentResult] = []\n        self.datasets: Dict[str, pd.Series] = {}\n        \n    def add_dataset(self, name: str, data: pd.Series, description: str = \"\"):\n        \"\"\"Aggiungi un dataset per il benchmarking.\"\"\"\n        self.datasets[name] = {\n            'data': data,\n            'description': description,\n            'length': len(data),\n            'frequency': data.index.freq if hasattr(data.index, 'freq') else 'Sconosciuta'\n        }\n        print(f\"📊 Dataset '{name}' aggiunto: {len(data)} osservazioni\")\n    \n    def evaluate_model(self, y_true: np.ndarray, y_pred: np.ndarray, \n                      y_pred_ci: Optional[np.ndarray] = None) -> Dict[str, float]:\n        \"\"\"Metriche complete di valutazione del modello.\"\"\"\n        metrics = {}\n        \n        # Metriche di accuratezza base\n        metrics['MAE'] = mean_absolute_error(y_true, y_pred)\n        metrics['RMSE'] = np.sqrt(mean_squared_error(y_true, y_pred))\n        metrics['MAPE'] = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n        \n        # MAPE simmetrico (gestisce meglio i valori zero)\n        metrics['SMAPE'] = np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100\n        \n        # R-squared (coefficiente di determinazione)\n        metrics['R2'] = r2_score(y_true, y_pred)\n        \n        # Accuratezza direzionale (per predizione trend)\n        if len(y_true) > 1:\n            true_directions = np.diff(y_true) > 0\n            pred_directions = np.diff(y_pred) > 0\n            metrics['Directional_Accuracy'] = np.mean(true_directions == pred_directions) * 100\n        \n        # Statistica U di Theil (accuratezza forecast relativa a forecast naïve)\n        naive_forecast = y_true[:-1]  # forecast lag-1\n        if len(naive_forecast) > 0:\n            mse_model = mean_squared_error(y_true[1:], y_pred[1:])\n            mse_naive = mean_squared_error(y_true[1:], naive_forecast)\n            metrics['Theil_U'] = np.sqrt(mse_model / mse_naive) if mse_naive > 0 else np.inf\n        \n        # Rapporto di copertura per intervalli di confidenza\n        if y_pred_ci is not None and len(y_pred_ci.shape) == 2:\n            lower_bound = y_pred_ci[:, 0]\n            upper_bound = y_pred_ci[:, 1]\n            coverage = np.mean((y_true >= lower_bound) & (y_true <= upper_bound))\n            metrics['CI_Coverage'] = coverage * 100\n            \n            # Larghezza media dell'intervallo\n            metrics['CI_Width'] = np.mean(upper_bound - lower_bound)\n        \n        return metrics\n    \n    def run_experiment(self, algorithm_func, algorithm_name: str, \n                      train_data: pd.Series, test_data: pd.Series,\n                      **algorithm_params) -> ExperimentResult:\n        \"\"\"Esegui un singolo esperimento algoritmico.\"\"\"\n        \n        print(f\"🔬 Eseguendo esperimento: {algorithm_name}\")\n        \n        # Misura tempo di training e memoria\n        import psutil\n        process = psutil.Process()\n        mem_before = process.memory_info().rss / 1024 / 1024  # MB\n        \n        start_time = time.time()\n        \n        try:\n            # Addestra modello\n            model, model_info = algorithm_func(train_data, **algorithm_params)\n            training_time = time.time() - start_time\n            \n            # Misura tempo di predizione\n            pred_start = time.time()\n            predictions, confidence_intervals = self._get_predictions(model, len(test_data), model_info)\n            prediction_time = time.time() - pred_start\n            \n            # Utilizzo memoria\n            mem_after = process.memory_info().rss / 1024 / 1024\n            memory_usage = mem_after - mem_before\n            \n            # Valuta performance\n            metrics = self.evaluate_model(test_data.values, predictions, confidence_intervals)\n            \n            # Importanza feature (se disponibile)\n            feature_importance = self._extract_feature_importance(model, model_info)\n            \n            result = ExperimentResult(\n                algorithm_name=algorithm_name,\n                parameters=algorithm_params,\n                training_time=training_time,\n                prediction_time=prediction_time,\n                memory_usage=memory_usage,\n                metrics=metrics,\n                predictions=predictions,\n                model_object=model,\n                feature_importance=feature_importance,\n                confidence_intervals=confidence_intervals\n            )\n            \n            self.results.append(result)\n            print(f\"✅ {algorithm_name} completato - RMSE: {metrics['RMSE']:.4f}\")\n            \n            return result\n            \n        except Exception as e:\n            print(f\"❌ {algorithm_name} fallito: {str(e)}\")\n            return None\n    \n    def _get_predictions(self, model, n_steps: int, model_info: Dict) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n        \"\"\"Estrai predizioni da diversi tipi di modello.\"\"\"\n        predictions = None\n        confidence_intervals = None\n        \n        model_type = model_info.get('type', 'unknown')\n        \n        if model_type in ['arima', 'sarima', 'exponential_smoothing']:\n            # Modelli statistici\n            if hasattr(model, 'forecast'):\n                if model_type in ['arima', 'sarima']:\n                    forecast_result = model.forecast(steps=n_steps, return_conf_int=True)\n                    if isinstance(forecast_result, tuple):\n                        predictions = forecast_result[0].values\n                        confidence_intervals = forecast_result[1].values\n                    else:\n                        predictions = forecast_result.values\n                else:\n                    predictions = model.forecast(steps=n_steps)\n            elif hasattr(model, 'predict'):\n                predictions = model.predict(start=len(model.model.endog), \n                                          end=len(model.model.endog) + n_steps - 1)\n        \n        elif model_type in ['sklearn', 'xgboost']:\n            # Modelli ML - necessario generare feature\n            X_test = model_info.get('X_test')\n            if X_test is not None:\n                predictions = model.predict(X_test)\n        \n        return predictions, confidence_intervals\n    \n    def _extract_feature_importance(self, model, model_info: Dict) -> Optional[Dict[str, float]]:\n        \"\"\"Estrai importanza feature se disponibile.\"\"\"\n        if hasattr(model, 'feature_importances_'):\n            feature_names = model_info.get('feature_names', [f'feature_{i}' for i in range(len(model.feature_importances_))])\n            return dict(zip(feature_names, model.feature_importances_))\n        return None\n\n# Inizializza sistema benchmark\nbenchmark = AlgorithmBenchmark(random_seed=RANDOM_SEED)\nprint(\"🏗️  Infrastruttura di ricerca pronta!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🔬 Implementazioni Algoritmi\n\nDefinisce implementazioni standardizzate di vari algoritmi di forecasting per un confronto equo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_lagged_features(series: pd.Series, n_lags: int = 12, \n                          include_stats: bool = True) -> pd.DataFrame:\n    \"\"\"Crea feature con ritardi per modelli ML.\"\"\"\n    features = pd.DataFrame(index=series.index)\n    \n    # Valori ritardati\n    for lag in range(1, n_lags + 1):\n        features[f'lag_{lag}'] = series.shift(lag)\n    \n    if include_stats:\n        # Statistiche rolling\n        for window in [3, 7, 30]:\n            if window <= len(series):\n                features[f'rolling_mean_{window}'] = series.rolling(window).mean().shift(1)\n                features[f'rolling_std_{window}'] = series.rolling(window).std().shift(1)\n        \n        # Feature basate sul tempo\n        if hasattr(series.index, 'dayofweek'):\n            features['dayofweek'] = series.index.dayofweek\n            features['month'] = series.index.month\n            features['quarter'] = series.index.quarter\n        \n        # Feature trend\n        features['trend'] = np.arange(len(series))\n    \n    return features.dropna()\n\n# Implementazioni algoritmi\ndef experiment_arima(data: pd.Series, order: Tuple[int, int, int] = (1, 1, 1)) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento modello ARIMA.\"\"\"\n    if PROJECT_IMPORTS_OK:\n        model = ARIMAForecaster(order=order)\n        model.fit(data)\n        return model, {'type': 'arima', 'order': order}\n    else:\n        model = ARIMA(data, order=order).fit()\n        return model, {'type': 'arima', 'order': order}\n\ndef experiment_sarima(data: pd.Series, order: Tuple[int, int, int] = (1, 1, 1),\n                     seasonal_order: Tuple[int, int, int, int] = (1, 1, 1, 12)) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento modello SARIMA.\"\"\"\n    if PROJECT_IMPORTS_OK:\n        model = SARIMAForecaster(order=order, seasonal_order=seasonal_order)\n        model.fit(data)\n        return model, {'type': 'sarima', 'order': order, 'seasonal_order': seasonal_order}\n    else:\n        model = SARIMAX(data, order=order, seasonal_order=seasonal_order).fit()\n        return model, {'type': 'sarima', 'order': order, 'seasonal_order': seasonal_order}\n\ndef experiment_exponential_smoothing(data: pd.Series, trend: str = 'add', \n                                   seasonal: str = 'add', seasonal_periods: int = 12) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento Exponential Smoothing.\"\"\"\n    model = ExponentialSmoothing(data, trend=trend, seasonal=seasonal, \n                               seasonal_periods=seasonal_periods).fit()\n    return model, {'type': 'exponential_smoothing', 'trend': trend, 'seasonal': seasonal}\n\ndef experiment_random_forest(data: pd.Series, n_estimators: int = 100, \n                           n_lags: int = 12, test_size: int = 12) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento Random Forest.\"\"\"\n    # Crea feature\n    features = create_lagged_features(data, n_lags=n_lags)\n    \n    # Prepara train/test\n    X = features.values[:-test_size]\n    y = data.iloc[n_lags:-test_size].values\n    X_test = features.values[-test_size:]\n    \n    # Addestra modello\n    model = RandomForestRegressor(n_estimators=n_estimators, random_state=RANDOM_SEED)\n    model.fit(X, y)\n    \n    return model, {\n        'type': 'sklearn', \n        'X_test': X_test,\n        'feature_names': features.columns.tolist()\n    }\n\ndef experiment_xgboost(data: pd.Series, n_estimators: int = 100, \n                      n_lags: int = 12, test_size: int = 12) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento XGBoost.\"\"\"\n    if not XGBOOST_AVAILABLE:\n        raise ImportError(\"XGBoost non disponibile\")\n    \n    # Crea feature\n    features = create_lagged_features(data, n_lags=n_lags)\n    \n    # Prepara train/test\n    X = features.values[:-test_size]\n    y = data.iloc[n_lags:-test_size].values\n    X_test = features.values[-test_size:]\n    \n    # Addestra modello\n    model = xgb.XGBRegressor(n_estimators=n_estimators, random_state=RANDOM_SEED)\n    model.fit(X, y)\n    \n    return model, {\n        'type': 'xgboost', \n        'X_test': X_test,\n        'feature_names': features.columns.tolist()\n    }\n\ndef experiment_svr(data: pd.Series, kernel: str = 'rbf', n_lags: int = 12, \n                  test_size: int = 12) -> Tuple[Any, Dict]:\n    \"\"\"Esperimento Support Vector Regression.\"\"\"\n    # Crea feature\n    features = create_lagged_features(data, n_lags=n_lags)\n    \n    # Prepara train/test\n    X = features.values[:-test_size]\n    y = data.iloc[n_lags:-test_size].values\n    X_test = features.values[-test_size:]\n    \n    # Scala feature per SVR\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Addestra modello\n    model = SVR(kernel=kernel)\n    model.fit(X_scaled, y)\n    \n    # Wrappa modello con scaler\n    class ScaledSVR:\n        def __init__(self, model, scaler):\n            self.model = model\n            self.scaler = scaler\n        \n        def predict(self, X):\n            return self.model.predict(self.scaler.transform(X))\n    \n    wrapped_model = ScaledSVR(model, scaler)\n    \n    return wrapped_model, {\n        'type': 'sklearn',\n        'X_test': X_test,  # Usa non scalati per consistenza\n        'feature_names': features.columns.tolist()\n    }\n\nprint(\"🧪 Implementazioni algoritmi pronte!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📊 Generazione e Preparazione Dataset\n\nCrea dataset sintetici diversi con caratteristiche differenti per testare la robustezza degli algoritmi."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_synthetic_ts(name: str, n_periods: int = 200, freq: str = 'D', \n                         trend_type: str = 'linear', seasonality: bool = True,\n                         noise_level: float = 0.1, anomalies: bool = False) -> pd.Series:\n    \"\"\"Genera serie temporali sintetiche con caratteristiche specificate.\"\"\"\n    \n    dates = pd.date_range('2020-01-01', periods=n_periods, freq=freq)\n    t = np.arange(n_periods)\n    \n    # Trend di base\n    if trend_type == 'linear':\n        trend = 0.1 * t + 100\n    elif trend_type == 'exponential':\n        trend = 100 * np.exp(0.001 * t)\n    elif trend_type == 'polynomial':\n        trend = 100 + 0.1 * t + 0.0001 * t**2\n    elif trend_type == 'none':\n        trend = np.full(n_periods, 100)\n    else:\n        trend = np.full(n_periods, 100)\n    \n    # Stagionalità\n    seasonal = np.zeros(n_periods)\n    if seasonality:\n        # Stagionalità annuale\n        seasonal += 10 * np.sin(2 * np.pi * t / 365.25)\n        # Stagionalità settimanale (se frequenza giornaliera)\n        if freq == 'D':\n            seasonal += 5 * np.sin(2 * np.pi * t / 7)\n        # Stagionalità mensile (se frequenza maggiore)\n        if freq in ['D', 'H']:\n            seasonal += 3 * np.sin(2 * np.pi * t / 30.44)\n    \n    # Rumore\n    noise = np.random.normal(0, noise_level * np.mean(trend), n_periods)\n    \n    # Combina componenti\n    series = trend + seasonal + noise\n    \n    # Aggiungi anomalie\n    if anomalies:\n        n_anomalies = max(1, n_periods // 50)  # ~2% anomalie\n        anomaly_idx = np.random.choice(n_periods, n_anomalies, replace=False)\n        for idx in anomaly_idx:\n            series[idx] *= np.random.choice([0.5, 1.5, 2.0])  # Moltiplicatore anomalia\n    \n    return pd.Series(series, index=dates, name=name)\n\n# Genera dataset di ricerca\nprint(\"📊 Generazione dataset di ricerca...\")\n\n# Dataset 1: Trend lineare semplice con stagionalità\nts_linear = generate_synthetic_ts(\n    'linear_seasonal', \n    n_periods=300, \n    trend_type='linear',\n    seasonality=True,\n    noise_level=0.05\n)\nbenchmark.add_dataset('Linear_Stagionale', ts_linear, 'Trend lineare con stagionalità annuale/settimanale')\n\n# Dataset 2: Crescita esponenziale\nts_exponential = generate_synthetic_ts(\n    'exponential_growth',\n    n_periods=200,\n    trend_type='exponential',\n    seasonality=False,\n    noise_level=0.1\n)\nbenchmark.add_dataset('Crescita_Esponenziale', ts_exponential, 'Pattern di crescita esponenziale')\n\n# Dataset 3: Rumore elevato con anomalie\nts_noisy = generate_synthetic_ts(\n    'noisy_anomalies',\n    n_periods=250,\n    trend_type='linear',\n    seasonality=True,\n    noise_level=0.3,\n    anomalies=True\n)\nbenchmark.add_dataset('Rumoroso_Anomalie', ts_noisy, 'Rumore elevato con anomalie')\n\n# Dataset 4: Serie stazionaria (senza trend)\nts_stationary = generate_synthetic_ts(\n    'stationary',\n    n_periods=200,\n    trend_type='none',\n    seasonality=True,\n    noise_level=0.1\n)\nbenchmark.add_dataset('Stazionaria', ts_stationary, 'Stazionaria con stagionalità')\n\n# Dataset 5: Trend polinomiale complesso\nts_polynomial = generate_synthetic_ts(\n    'polynomial',\n    n_periods=300,\n    trend_type='polynomial',\n    seasonality=True,\n    noise_level=0.08\n)\nbenchmark.add_dataset('Polinomiale', ts_polynomial, 'Trend polinomiale con stagionalità')\n\n# Visualizza dataset\nfig, axes = plt.subplots(3, 2, figsize=(15, 12))\naxes = axes.ravel()\n\nfor i, (name, dataset) in enumerate(benchmark.datasets.items()):\n    if i < 5:\n        ax = axes[i]\n        data = dataset['data']\n        ax.plot(data.index, data.values, linewidth=1.5, alpha=0.8)\n        ax.set_title(f'{name}\\n{dataset[\"description\"]}', fontsize=10)\n        ax.grid(True, alpha=0.3)\n        ax.tick_params(axis='x', rotation=45)\n\n# Rimuovi subplot vuoto\naxes[5].remove()\n\nplt.tight_layout()\nplt.savefig(plots_dir / 'dataset_ricerca.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"📊 Generati {len(benchmark.datasets)} dataset di ricerca\")\nfor name, dataset in benchmark.datasets.items():\n    print(f\"  • {name}: {dataset['length']} osservazioni - {dataset['description']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🧪 Esperimenti Algoritmi Completi\n\nEsegue esperimenti sistematici attraverso tutti gli algoritmi e dataset con valutazione rigorosa."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configurazione esperimenti\nTEST_SIZE = 30  # Numero di periodi per test out-of-sample\nexperiments_config = [\n    ('ARIMA_111', experiment_arima, {'order': (1, 1, 1)}),\n    ('ARIMA_212', experiment_arima, {'order': (2, 1, 2)}),\n    ('SARIMA_111_1111_12', experiment_sarima, {\n        'order': (1, 1, 1), \n        'seasonal_order': (1, 1, 1, 12)\n    }),\n    ('ExpSmoothing_Add', experiment_exponential_smoothing, {\n        'trend': 'add', \n        'seasonal': 'add', \n        'seasonal_periods': 12\n    }),\n    ('RandomForest_100', experiment_random_forest, {\n        'n_estimators': 100, \n        'n_lags': 12, \n        'test_size': TEST_SIZE\n    }),\n    ('RandomForest_500', experiment_random_forest, {\n        'n_estimators': 500, \n        'n_lags': 20, \n        'test_size': TEST_SIZE\n    }),\n]\n\n# Aggiungi XGBoost se disponibile\nif XGBOOST_AVAILABLE:\n    experiments_config.extend([\n        ('XGBoost_100', experiment_xgboost, {\n            'n_estimators': 100, \n            'n_lags': 12, \n            'test_size': TEST_SIZE\n        }),\n        ('XGBoost_300', experiment_xgboost, {\n            'n_estimators': 300, \n            'n_lags': 20, \n            'test_size': TEST_SIZE\n        }),\n    ])\n\n# Aggiungi SVR se disponibile\nif SVR_AVAILABLE:\n    experiments_config.extend([\n        ('SVR_RBF', experiment_svr, {\n            'kernel': 'rbf', \n            'n_lags': 12, \n            'test_size': TEST_SIZE\n        }),\n        ('SVR_Linear', experiment_svr, {\n            'kernel': 'linear', \n            'n_lags': 12, \n            'test_size': TEST_SIZE\n        }),\n    ])\n\nprint(f\"🔬 Esecuzione di {len(experiments_config)} algoritmi su {len(benchmark.datasets)} dataset\")\nprint(f\"📊 Esperimenti totali: {len(experiments_config) * len(benchmark.datasets)}\")\n\n# Esegui tutti gli esperimenti\nresults_matrix = []\nexperiment_counter = 0\ntotal_experiments = len(experiments_config) * len(benchmark.datasets)\n\nfor dataset_name, dataset_info in benchmark.datasets.items():\n    print(f\"\\n📊 Dataset: {dataset_name}\")\n    print(\"=\" * 50)\n    \n    data = dataset_info['data']\n    train_data = data[:-TEST_SIZE]\n    test_data = data[-TEST_SIZE:]\n    \n    dataset_results = []\n    \n    for algo_name, algo_func, algo_params in experiments_config:\n        experiment_counter += 1\n        print(f\"\\n[{experiment_counter}/{total_experiments}] {algo_name} su {dataset_name}\")\n        \n        try:\n            result = benchmark.run_experiment(\n                algo_func, algo_name, train_data, test_data, **algo_params\n            )\n            \n            if result:\n                dataset_results.append(result)\n                \n                # Aggiungi alla matrice risultati\n                results_matrix.append({\n                    'Dataset': dataset_name,\n                    'Algoritmo': algo_name,\n                    'RMSE': result.metrics['RMSE'],\n                    'MAE': result.metrics['MAE'],\n                    'MAPE': result.metrics['MAPE'],\n                    'R2': result.metrics['R2'],\n                    'Tempo_Training': result.training_time,\n                    'Tempo_Predizione': result.prediction_time,\n                    'Memoria_MB': result.memory_usage\n                })\n            \n        except Exception as e:\n            print(f\"⚠️  Esperimento fallito: {e}\")\n            continue\n\nprint(f\"\\n✅ Completati {len(results_matrix)} esperimenti con successo\")\n\n# Crea DataFrame risultati\nresults_df = pd.DataFrame(results_matrix)\nprint(f\"📊 Matrice risultati shape: {results_df.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📈 Analisi Performance e Visualizzazione\n\nAnalisi completa dei risultati sperimentali con visualizzazioni professionali."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Statistiche Riassuntive Performance\nprint(\"📊 RIASSUNTO RISULTATI ESPERIMENTI\")\nprint(\"=\" * 60)\n\nif len(results_df) > 0:\n    # Classifica performance generale\n    print(\"\\n🏆 Classifica Algoritmi Generale (per RMSE Medio):\")\n    avg_performance = results_df.groupby('Algoritmo').agg({\n        'RMSE': ['mean', 'std', 'min', 'max'],\n        'MAE': ['mean', 'std'],\n        'R2': ['mean', 'std'],\n        'Tempo_Training': ['mean', 'std'],\n        'Memoria_MB': ['mean', 'std']\n    }).round(4)\n    \n    avg_performance.columns = ['_'.join(col).strip() for col in avg_performance.columns]\n    ranking = avg_performance.sort_values('RMSE_mean')\n    \n    for i, (algo, row) in enumerate(ranking.iterrows(), 1):\n        print(f\"{i:2d}. {algo:20s} - RMSE: {row['RMSE_mean']:8.4f} (±{row['RMSE_std']:6.4f}) | \"\n              f\"R²: {row['R2_mean']:6.3f} | Tempo: {row['Tempo_Training_mean']:6.2f}s\")\n    \n    # Analisi difficoltà dataset\n    print(\"\\n📊 Classifica Difficoltà Dataset (per RMSE Medio):\")\n    dataset_difficulty = results_df.groupby('Dataset')['RMSE'].agg(['mean', 'std']).round(4)\n    dataset_difficulty = dataset_difficulty.sort_values('mean', ascending=False)\n    \n    for i, (dataset, row) in enumerate(dataset_difficulty.iterrows(), 1):\n        print(f\"{i}. {dataset:20s} - RMSE Medio: {row['mean']:8.4f} (±{row['std']:6.4f})\")\n    \n    # Miglior algoritmo per dataset\n    print(\"\\n🎯 Miglior Algoritmo per Dataset:\")\n    best_per_dataset = results_df.loc[results_df.groupby('Dataset')['RMSE'].idxmin()]\n    \n    for _, row in best_per_dataset.iterrows():\n        print(f\"{row['Dataset']:20s} → {row['Algoritmo']:20s} (RMSE: {row['RMSE']:7.4f})\")\n    \n    # Analisi Performance vs Efficienza\n    print(\"\\n⚡ Analisi Performance vs Efficienza:\")\n    efficiency_df = results_df.groupby('Algoritmo').agg({\n        'RMSE': 'mean',\n        'Tempo_Training': 'mean',\n        'Memoria_MB': 'mean'\n    }).round(4)\n    \n    # Normalizza metriche per confronto (più basso è meglio per tutti)\n    efficiency_df['RMSE_norm'] = (efficiency_df['RMSE'] - efficiency_df['RMSE'].min()) / (efficiency_df['RMSE'].max() - efficiency_df['RMSE'].min())\n    efficiency_df['Tempo_norm'] = (efficiency_df['Tempo_Training'] - efficiency_df['Tempo_Training'].min()) / (efficiency_df['Tempo_Training'].max() - efficiency_df['Tempo_Training'].min())\n    efficiency_df['Memoria_norm'] = (efficiency_df['Memoria_MB'] - efficiency_df['Memoria_MB'].min()) / (efficiency_df['Memoria_MB'].max() - efficiency_df['Memoria_MB'].min())\n    \n    # Punteggio efficienza combinato (più basso è meglio)\n    efficiency_df['Punteggio_Efficienza'] = (efficiency_df['RMSE_norm'] + efficiency_df['Tempo_norm'] + efficiency_df['Memoria_norm']) / 3\n    efficiency_ranking = efficiency_df.sort_values('Punteggio_Efficienza')\n    \n    for i, (algo, row) in enumerate(efficiency_ranking.iterrows(), 1):\n        print(f\"{i:2d}. {algo:20s} - Punteggio: {row['Punteggio_Efficienza']:5.3f} | \"\n              f\"RMSE: {row['RMSE']:7.4f} | Tempo: {row['Tempo_Training']:6.2f}s | Memoria: {row['Memoria_MB']:5.1f}MB\")\n\nelse:\n    print(\"❌ Nessun esperimento riuscito da analizzare\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizzazioni Avanzate\nif len(results_df) > 0:\n    \n    # Crea dashboard di visualizzazione completo\n    fig = plt.figure(figsize=(20, 16))\n    \n    # 1. Mappa di Calore Performance\n    ax1 = plt.subplot(3, 3, 1)\n    pivot_rmse = results_df.pivot(index='Algoritmo', columns='Dataset', values='RMSE')\n    sns.heatmap(pivot_rmse, annot=True, fmt='.3f', cmap='RdYlBu_r', ax=ax1)\n    ax1.set_title('Mappa di Calore Performance RMSE\\n(Più Basso è Meglio)', fontweight='bold')\n    ax1.set_xlabel('Dataset')\n    ax1.set_ylabel('Algoritmo')\n    \n    # 2. Distribuzione Performance Algoritmi\n    ax2 = plt.subplot(3, 3, 2)\n    results_df.boxplot(column='RMSE', by='Algoritmo', ax=ax2)\n    ax2.set_title('Distribuzione RMSE per Algoritmo')\n    ax2.set_xlabel('Algoritmo')\n    ax2.set_ylabel('RMSE')\n    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n    plt.suptitle('')  # Rimuovi titolo automatico\n    \n    # 3. Tempo Training vs Performance\n    ax3 = plt.subplot(3, 3, 3)\n    scatter = ax3.scatter(results_df['Tempo_Training'], results_df['RMSE'], \n                         c=results_df['R2'], s=50, alpha=0.7, cmap='viridis')\n    ax3.set_xlabel('Tempo Training (secondi)')\n    ax3.set_ylabel('RMSE')\n    ax3.set_title('Performance vs Tempo Training\\n(Colore: R²)')\n    plt.colorbar(scatter, ax=ax3, label='R²')\n    \n    # 4. Analisi Utilizzo Memoria\n    ax4 = plt.subplot(3, 3, 4)\n    memory_by_algo = results_df.groupby('Algoritmo')['Memoria_MB'].mean().sort_values()\n    bars = ax4.bar(range(len(memory_by_algo)), memory_by_algo.values)\n    ax4.set_xticks(range(len(memory_by_algo)))\n    ax4.set_xticklabels(memory_by_algo.index, rotation=45, ha='right')\n    ax4.set_ylabel('Utilizzo Memoria (MB)')\n    ax4.set_title('Utilizzo Memoria Medio per Algoritmo')\n    \n    # Aggiungi etichette valori sulle barre\n    for i, (bar, val) in enumerate(zip(bars, memory_by_algo.values)):\n        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n                f'{val:.1f}', ha='center', va='bottom', fontsize=8)\n    \n    # 5. Analisi Difficoltà Dataset\n    ax5 = plt.subplot(3, 3, 5)\n    dataset_stats = results_df.groupby('Dataset')['RMSE'].agg(['mean', 'std'])\n    ax5.errorbar(range(len(dataset_stats)), dataset_stats['mean'], \n                yerr=dataset_stats['std'], fmt='o-', capsize=5, markersize=8)\n    ax5.set_xticks(range(len(dataset_stats)))\n    ax5.set_xticklabels(dataset_stats.index, rotation=45, ha='right')\n    ax5.set_ylabel('RMSE')\n    ax5.set_title('Difficoltà Dataset\\n(RMSE Medio ± Std)')\n    ax5.grid(True, alpha=0.3)\n    \n    # 6. Confronto Performance R²\n    ax6 = plt.subplot(3, 3, 6)\n    r2_by_algo = results_df.groupby('Algoritmo')['R2'].mean().sort_values(ascending=False)\n    bars = ax6.bar(range(len(r2_by_algo)), r2_by_algo.values, color='lightcoral')\n    ax6.set_xticks(range(len(r2_by_algo)))\n    ax6.set_xticklabels(r2_by_algo.index, rotation=45, ha='right')\n    ax6.set_ylabel('Punteggio R²')\n    ax6.set_title('Punteggio R² Medio per Algoritmo')\n    ax6.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n    \n    # 7. Grafico Radar Multi-Metrica (per i top 5 algoritmi)\n    ax7 = plt.subplot(3, 3, 7, projection='polar')\n    \n    # Normalizza metriche per grafico radar\n    top_5_algos = results_df.groupby('Algoritmo')['RMSE'].mean().nsmallest(5).index\n    radar_data = results_df[results_df['Algoritmo'].isin(top_5_algos)].groupby('Algoritmo').agg({\n        'RMSE': 'mean',\n        'MAE': 'mean', \n        'R2': 'mean',\n        'Tempo_Training': 'mean',\n        'Memoria_MB': 'mean'\n    })\n    \n    # Normalizza (inverti per RMSE, MAE, Tempo, Memoria così più alto è meglio)\n    radar_normalized = radar_data.copy()\n    for col in ['RMSE', 'MAE', 'Tempo_Training', 'Memoria_MB']:\n        radar_normalized[col] = 1 - (radar_data[col] - radar_data[col].min()) / (radar_data[col].max() - radar_data[col].min())\n    \n    # R2 è già più-alto-è-meglio\n    radar_normalized['R2'] = (radar_data['R2'] - radar_data['R2'].min()) / (radar_data['R2'].max() - radar_data['R2'].min())\n    \n    categories = ['RMSE', 'MAE', 'R²', 'Velocità', 'Memoria']\n    N = len(categories)\n    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n    angles += angles[:1]  # Completa il cerchio\n    \n    colors = plt.cm.Set3(np.linspace(0, 1, len(top_5_algos)))\n    \n    for i, (algo, row) in enumerate(radar_normalized.iterrows()):\n        values = row.tolist()\n        values += values[:1]  # Completa il cerchio\n        ax7.plot(angles, values, 'o-', linewidth=2, label=algo, color=colors[i])\n        ax7.fill(angles, values, alpha=0.25, color=colors[i])\n    \n    ax7.set_xticks(angles[:-1])\n    ax7.set_xticklabels(categories)\n    ax7.set_ylim(0, 1)\n    ax7.set_title('Performance Multi-Metrica\\n(Top 5 Algoritmi)', y=1.08)\n    ax7.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n    \n    # 8. Scatter Accuratezza Predizione\n    ax8 = plt.subplot(3, 3, 8)\n    for algo in results_df['Algoritmo'].unique():\n        algo_data = results_df[results_df['Algoritmo'] == algo]\n        ax8.scatter(algo_data['MAE'], algo_data['MAPE'], label=algo, alpha=0.7, s=50)\n    \n    ax8.set_xlabel('MAE')\n    ax8.set_ylabel('MAPE (%)')\n    ax8.set_title('MAE vs MAPE per Algoritmo')\n    ax8.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    ax8.grid(True, alpha=0.3)\n    \n    # 9. Distribuzione Tempo Training\n    ax9 = plt.subplot(3, 3, 9)\n    results_df['Tempo_Training'].hist(bins=20, alpha=0.7, ax=ax9)\n    ax9.axvline(results_df['Tempo_Training'].mean(), color='red', linestyle='--', \n                label=f'Media: {results_df[\"Tempo_Training\"].mean():.2f}s')\n    ax9.set_xlabel('Tempo Training (secondi)')\n    ax9.set_ylabel('Frequenza')\n    ax9.set_title('Distribuzione Tempo Training')\n    ax9.legend()\n    ax9.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(plots_dir / 'analisi_completa.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"📊 Visualizzazione analisi completa salvata in {plots_dir / 'analisi_completa.png'}\")\n    \nelse:\n    print(\"❌ Nessun risultato da visualizzare\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🔍 Analisi Approfondita: Importanza Feature e Interpretabilità Modelli\n\nAnalizza quali feature e pattern i diversi algoritmi stanno apprendendo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analisi Importanza Feature per Modelli Basati su Albero\nprint(\"🔍 ANALISI IMPORTANZA FEATURE\")\nprint(\"=\" * 50)\n\n# Raccogli importanza feature dagli esperimenti riusciti\nfeature_importance_data = []\n\nfor result in benchmark.results:\n    if result.feature_importance is not None:\n        for feature, importance in result.feature_importance.items():\n            feature_importance_data.append({\n                'Algoritmo': result.algorithm_name,\n                'Feature': feature,\n                'Importanza': importance\n            })\n\nif feature_importance_data:\n    fi_df = pd.DataFrame(feature_importance_data)\n    \n    # Importanza feature media per algoritmo\n    avg_importance = fi_df.groupby(['Algoritmo', 'Feature'])['Importanza'].mean().reset_index()\n    \n    # Visualizza importanza feature\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # Top feature complessive\n    ax1 = axes[0, 0]\n    top_features = fi_df.groupby('Feature')['Importanza'].mean().sort_values(ascending=False).head(15)\n    bars = ax1.bar(range(len(top_features)), top_features.values)\n    ax1.set_xticks(range(len(top_features)))\n    ax1.set_xticklabels(top_features.index, rotation=45, ha='right')\n    ax1.set_ylabel('Importanza Media')\n    ax1.set_title('Top 15 Feature Più Importanti (Complessivo)')\n    ax1.grid(True, alpha=0.3)\n    \n    # Importanza feature per algoritmo\n    ax2 = axes[0, 1]\n    algorithms_with_fi = avg_importance['Algoritmo'].unique()\n    if len(algorithms_with_fi) > 0:\n        pivot_fi = avg_importance.pivot(index='Feature', columns='Algoritmo', values='Importanza')\n        # Seleziona top 10 feature per chiarezza\n        top_10_features = pivot_fi.sum(axis=1).sort_values(ascending=False).head(10).index\n        pivot_fi_top = pivot_fi.loc[top_10_features]\n        \n        sns.heatmap(pivot_fi_top, annot=True, fmt='.3f', cmap='Blues', ax=ax2)\n        ax2.set_title('Importanza Feature per Algoritmo\\n(Top 10 Feature)')\n        ax2.set_xlabel('Algoritmo')\n        ax2.set_ylabel('Feature')\n    \n    # Analisi tipo feature\n    ax3 = axes[1, 0]\n    \n    def categorize_feature(feature_name):\n        if 'lag_' in feature_name:\n            return 'Valori Ritardati'\n        elif 'rolling_mean' in feature_name:\n            return 'Media Mobile'\n        elif 'rolling_std' in feature_name:\n            return 'Deviazione Std Mobile'\n        elif feature_name in ['dayofweek', 'month', 'quarter']:\n            return 'Feature Temporali'\n        elif feature_name == 'trend':\n            return 'Trend'\n        else:\n            return 'Altro'\n    \n    fi_df['Tipo_Feature'] = fi_df['Feature'].apply(categorize_feature)\n    feature_type_importance = fi_df.groupby('Tipo_Feature')['Importanza'].mean().sort_values(ascending=False)\n    \n    colors = plt.cm.Set3(np.arange(len(feature_type_importance)))\n    wedges, texts, autotexts = ax3.pie(feature_type_importance.values, \n                                      labels=feature_type_importance.index,\n                                      autopct='%1.1f%%', \n                                      colors=colors,\n                                      startangle=90)\n    ax3.set_title('Importanza Feature per Tipo\\n(Media attraverso tutti gli algoritmi)')\n    \n    # Analisi importanza lag\n    ax4 = axes[1, 1]\n    lag_features = fi_df[fi_df['Feature'].str.contains('lag_')].copy()\n    if not lag_features.empty:\n        lag_features['Numero_Lag'] = lag_features['Feature'].str.extract(r'lag_(\\d+)').astype(int)\n        lag_importance = lag_features.groupby('Numero_Lag')['Importanza'].mean().sort_index()\n        \n        ax4.plot(lag_importance.index, lag_importance.values, 'o-', linewidth=2, markersize=6)\n        ax4.set_xlabel('Numero Lag')\n        ax4.set_ylabel('Importanza Media')\n        ax4.set_title('Importanza per Numero Lag')\n        ax4.grid(True, alpha=0.3)\n        \n        # Evidenzia lag più importanti\n        top_3_lags = lag_importance.nlargest(3)\n        for lag, importance in top_3_lags.items():\n            ax4.annotate(f'Lag {lag}', (lag, importance), \n                        xytext=(5, 5), textcoords='offset points',\n                        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n    \n    plt.tight_layout()\n    plt.savefig(plots_dir / 'analisi_importanza_feature.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # Stampa insight principali\n    print(\"\\n🎯 INSIGHT CHIAVE:\")\n    print(f\"\\n1. Feature Più Importanti (Top 5):\")\n    for i, (feature, importance) in enumerate(top_features.head().items(), 1):\n        print(f\"   {i}. {feature:20s}: {importance:.4f}\")\n    \n    print(f\"\\n2. Classifiche Tipo Feature:\")\n    for i, (ftype, importance) in enumerate(feature_type_importance.items(), 1):\n        print(f\"   {i}. {ftype:15s}: {importance:.4f}\")\n    \n    if not lag_features.empty:\n        print(f\"\\n3. Lag Più Importanti:\")\n        for i, (lag, importance) in enumerate(top_3_lags.items(), 1):\n            print(f\"   {i}. Lag {lag:2d}        : {importance:.4f}\")\n\nelse:\n    print(\"❌ Nessun dato importanza feature disponibile\")\n    print(\"   Questo accade di solito quando vengono testati solo modelli statistici\")\n    print(\"   Prova ad eseguire modelli basati su albero (RandomForest, XGBoost) per analisi feature\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🚀 Ricerca Avanzata: Metodi Ensemble e Approcci Ibridi\n\nSperimenta con la combinazione di diversi algoritmi per prestazioni migliorate."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"🚀 RICERCA AVANZATA: METODI ENSEMBLE\")\nprint(\"=\" * 50)\n\nif len(benchmark.results) >= 2:\n    \n    # Raggruppa risultati per dataset per creazione ensemble\n    ensemble_results = []\n    \n    for dataset_name in benchmark.datasets.keys():\n        print(f\"\\n📊 Creazione ensemble per {dataset_name}...\")\n        \n        # Ottieni tutti i risultati per questo dataset\n        dataset_results = []\n        for result in benchmark.results:\n            # Accoppia risultati al dataset (accoppiamento semplificato)\n            if result.predictions is not None and len(result.predictions) == TEST_SIZE:\n                dataset_results.append(result)\n        \n        if len(dataset_results) >= 2:\n            print(f\"   Trovati {len(dataset_results)} modelli per ensemble\")\n            \n            # Ottieni dati test per valutazione\n            test_data = benchmark.datasets[dataset_name]['data'][-TEST_SIZE:]\n            y_true = test_data.values\n            \n            # 1. Ensemble Media Semplice\n            predictions_matrix = np.column_stack([r.predictions for r in dataset_results])\n            ensemble_simple = np.mean(predictions_matrix, axis=1)\n            \n            # 2. Media Pesata (pesi RMSE inverso)\n            weights = []\n            for result in dataset_results:\n                rmse = result.metrics['RMSE']\n                weight = 1.0 / rmse if rmse > 0 else 1.0\n                weights.append(weight)\n            \n            weights = np.array(weights)\n            weights = weights / np.sum(weights)  # Normalizza\n            \n            ensemble_weighted = np.average(predictions_matrix, axis=1, weights=weights)\n            \n            # 3. Ensemble Migliori N Modelli (top 3)\n            best_n = min(3, len(dataset_results))\n            sorted_results = sorted(dataset_results, key=lambda x: x.metrics['RMSE'])\n            best_predictions = np.column_stack([r.predictions for r in sorted_results[:best_n]])\n            ensemble_best_n = np.mean(best_predictions, axis=1)\n            \n            # Valuta metodi ensemble\n            ensemble_methods = {\n                'Media_Semplice': ensemble_simple,\n                'Media_Pesata': ensemble_weighted,\n                f'Migliori_{best_n}_Media': ensemble_best_n\n            }\n            \n            for method_name, predictions in ensemble_methods.items():\n                metrics = benchmark.evaluate_model(y_true, predictions)\n                \n                ensemble_results.append({\n                    'Dataset': dataset_name,\n                    'Metodo': f'Ensemble_{method_name}',\n                    'RMSE': metrics['RMSE'],\n                    'MAE': metrics['MAE'],\n                    'MAPE': metrics['MAPE'],\n                    'R2': metrics['R2'],\n                    'Predizioni': predictions\n                })\n                \n                print(f\"   {method_name:20s}: RMSE = {metrics['RMSE']:.4f}, R² = {metrics['R2']:.4f}\")\n    \n    if ensemble_results:\n        ensemble_df = pd.DataFrame(ensemble_results)\n        \n        print(\"\\n🏆 RIASSUNTO PERFORMANCE ENSEMBLE\")\n        print(\"=\" * 40)\n        \n        # Confronta ensemble vs modelli individuali\n        print(\"\\n📊 Ensemble vs Miglior Modello Individuale:\")\n        \n        for dataset_name in ensemble_df['Dataset'].unique():\n            print(f\"\\n{dataset_name}:\")\n            \n            # Miglior modello individuale per questo dataset\n            individual_results = [r for r in benchmark.results \n                                if len(r.predictions) == TEST_SIZE and r.predictions is not None]\n            if individual_results:\n                best_individual = min(individual_results, key=lambda x: x.metrics['RMSE'])\n                print(f\"   Miglior Individuale  : {best_individual.algorithm_name:20s} RMSE = {best_individual.metrics['RMSE']:.4f}\")\n            \n            # Miglior ensemble per questo dataset\n            dataset_ensembles = ensemble_df[ensemble_df['Dataset'] == dataset_name]\n            best_ensemble = dataset_ensembles.loc[dataset_ensembles['RMSE'].idxmin()]\n            print(f\"   Miglior Ensemble     : {best_ensemble['Metodo']:20s} RMSE = {best_ensemble['RMSE']:.4f}\")\n            \n            if individual_results:\n                improvement = (best_individual.metrics['RMSE'] - best_ensemble['RMSE']) / best_individual.metrics['RMSE'] * 100\n                print(f\"   Miglioramento        : {improvement:+6.2f}%\")\n        \n        # Visualizza performance ensemble\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        \n        # Confronto metodi ensemble\n        ax1 = axes[0, 0]\n        ensemble_perf = ensemble_df.groupby('Metodo')['RMSE'].mean().sort_values()\n        bars = ax1.bar(range(len(ensemble_perf)), ensemble_perf.values)\n        ax1.set_xticks(range(len(ensemble_perf)))\n        ax1.set_xticklabels([m.replace('Ensemble_', '') for m in ensemble_perf.index], rotation=45)\n        ax1.set_ylabel('RMSE Medio')\n        ax1.set_title('Confronto Metodi Ensemble')\n        ax1.grid(True, alpha=0.3)\n        \n        # Scatter Ensemble vs Individuale\n        ax2 = axes[0, 1]\n        \n        # Raccogli performance modelli individuali\n        individual_rmse = []\n        ensemble_rmse = []\n        \n        for dataset_name in ensemble_df['Dataset'].unique():\n            # Miglior individuale\n            individual_results = [r for r in benchmark.results \n                                if len(r.predictions) == TEST_SIZE and r.predictions is not None]\n            if individual_results:\n                best_individual = min(individual_results, key=lambda x: x.metrics['RMSE'])\n                individual_rmse.append(best_individual.metrics['RMSE'])\n                \n                # Miglior ensemble\n                dataset_ensembles = ensemble_df[ensemble_df['Dataset'] == dataset_name]\n                best_ensemble_rmse = dataset_ensembles['RMSE'].min()\n                ensemble_rmse.append(best_ensemble_rmse)\n        \n        if individual_rmse and ensemble_rmse:\n            ax2.scatter(individual_rmse, ensemble_rmse, s=100, alpha=0.7)\n            \n            # Aggiungi linea diagonale (y=x)\n            min_val = min(min(individual_rmse), min(ensemble_rmse))\n            max_val = max(max(individual_rmse), max(ensemble_rmse))\n            ax2.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='Performance Uguale')\n            \n            ax2.set_xlabel('RMSE Miglior Individuale')\n            ax2.set_ylabel('RMSE Miglior Ensemble')\n            ax2.set_title('Performance Ensemble vs Individuale\\n(Sotto linea = Ensemble Migliore)')\n            ax2.legend()\n            ax2.grid(True, alpha=0.3)\n        \n        # Confronto predizioni per un dataset\n        ax3 = axes[1, 0]\n        if len(ensemble_results) > 0:\n            sample_result = ensemble_results[0]\n            sample_dataset = sample_result['Dataset']\n            test_data = benchmark.datasets[sample_dataset]['data'][-TEST_SIZE:]\n            \n            ax3.plot(test_data.index, test_data.values, 'k-', linewidth=2, label='Reale', alpha=0.8)\n            \n            # Plotta predizioni ensemble\n            dataset_ensembles = [r for r in ensemble_results if r['Dataset'] == sample_dataset]\n            colors = plt.cm.Set1(np.linspace(0, 1, len(dataset_ensembles)))\n            \n            for i, result in enumerate(dataset_ensembles):\n                ax3.plot(test_data.index, result['Predizioni'], '--', \n                        color=colors[i], label=result['Metodo'].replace('Ensemble_', ''), alpha=0.7)\n            \n            ax3.set_title(f'Confronto Predizioni: {sample_dataset}')\n            ax3.set_ylabel('Valore')\n            ax3.legend()\n            ax3.grid(True, alpha=0.3)\n        \n        # Distribuzione pesi per ensemble pesato\n        ax4 = axes[1, 1]\n        if len(dataset_results) > 0:\n            algo_names = [r.algorithm_name for r in dataset_results]\n            bars = ax4.bar(range(len(weights)), weights)\n            ax4.set_xticks(range(len(weights)))\n            ax4.set_xticklabels(algo_names, rotation=45, ha='right')\n            ax4.set_ylabel('Peso')\n            ax4.set_title('Pesi Ensemble\\n(Basato su RMSE Inverso)')\n            ax4.grid(True, alpha=0.3)\n            \n            # Aggiungi valori pesi sulle barre\n            for i, (bar, weight) in enumerate(zip(bars, weights)):\n                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n                        f'{weight:.3f}', ha='center', va='bottom', fontsize=9)\n        \n        plt.tight_layout()\n        plt.savefig(plots_dir / 'analisi_ensemble.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        \n    else:\n        print(\"❌ Nessun risultato ensemble generato\")\n        \nelse:\n    print(\"❌ Servono almeno 2 esperimenti riusciti per metodi ensemble\")\n    print(\"   Esegui prima più esperimenti algoritmici\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 📋 Riassunto Ricerca ed Export\n\nRiassume tutti i risultati ed esporta i risultati per ulteriori analisi o reportistica."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"📋 RIASSUNTO RICERCA ED EXPORT\")\nprint(\"=\" * 50)\n\n# Crea riassunto ricerca completo\nresearch_summary = {\n    'info_esperimento': {\n        'timestamp': pd.Timestamp.now().isoformat(),\n        'esperimenti_totali': len(benchmark.results),\n        'dataset_testati': len(benchmark.datasets),\n        'algoritmi_testati': len(set(r.algorithm_name for r in benchmark.results)),\n        'dimensione_test': TEST_SIZE,\n        'random_seed': RANDOM_SEED\n    },\n    'risultati_chiave': {},\n    'raccomandazioni': []\n}\n\nif len(results_df) > 0:\n    # Risultati chiave\n    best_overall = results_df.loc[results_df['RMSE'].idxmin()]\n    worst_overall = results_df.loc[results_df['RMSE'].idxmax()]\n    \n    research_summary['risultati_chiave'] = {\n        'miglior_algoritmo_complessivo': best_overall['Algoritmo'],\n        'miglior_performance_rmse': float(best_overall['RMSE']),\n        'peggior_algoritmo_complessivo': worst_overall['Algoritmo'],\n        'peggior_performance_rmse': float(worst_overall['RMSE']),\n        'range_performance': float(worst_overall['RMSE'] - best_overall['RMSE']),\n        'tempo_training_medio': float(results_df['Tempo_Training'].mean()),\n        'utilizzo_memoria_medio': float(results_df['Memoria_MB'].mean())\n    }\n    \n    # Classifiche algoritmi\n    algo_ranking = results_df.groupby('Algoritmo')['RMSE'].mean().sort_values()\n    research_summary['classifica_algoritmi'] = {\n        str(algo): float(rmse) for algo, rmse in algo_ranking.items()\n    }\n    \n    # Classifica difficoltà dataset\n    dataset_ranking = results_df.groupby('Dataset')['RMSE'].mean().sort_values(ascending=False)\n    research_summary['difficolta_dataset'] = {\n        str(dataset): float(rmse) for dataset, rmse in dataset_ranking.items()\n    }\n    \n    # Genera raccomandazioni\n    recommendations = []\n    \n    # Raccomandazioni performance\n    top_3_algos = algo_ranking.head(3)\n    recommendations.append(f\"Per massima accuratezza, considera: {', '.join(top_3_algos.index)}\")\n    \n    # Raccomandazioni velocità\n    fast_algos = results_df[results_df['Tempo_Training'] < results_df['Tempo_Training'].median()]\n    if not fast_algos.empty:\n        best_fast = fast_algos.loc[fast_algos['RMSE'].idxmin()]['Algoritmo']\n        recommendations.append(f\"Per equilibrio velocità-accuratezza, considera: {best_fast}\")\n    \n    # Raccomandazioni memoria\n    low_memory = results_df[results_df['Memoria_MB'] < results_df['Memoria_MB'].median()]\n    if not low_memory.empty:\n        best_memory = low_memory.loc[low_memory['RMSE'].idxmin()]['Algoritmo']\n        recommendations.append(f\"Per efficienza memoria, considera: {best_memory}\")\n    \n    # Raccomandazioni specifiche dataset\n    hardest_dataset = dataset_ranking.index[0]\n    best_on_hardest = results_df[results_df['Dataset'] == hardest_dataset].loc[\n        results_df[results_df['Dataset'] == hardest_dataset]['RMSE'].idxmin()\n    ]['Algoritmo']\n    recommendations.append(f\"Per dataset difficili (come {hardest_dataset}), considera: {best_on_hardest}\")\n    \n    research_summary['raccomandazioni'] = recommendations\n\n# Salva risultati dettagliati\ntimestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n\n# 1. Salva DataFrame risultati\nif len(results_df) > 0:\n    results_file = output_dir / f'risultati_esperimenti_{timestamp}.csv'\n    results_df.to_csv(results_file, index=False)\n    print(f\"📊 Risultati dettagliati salvati in: {results_file}\")\n\n# 2. Salva riassunto ricerca\nsummary_file = output_dir / f'riassunto_ricerca_{timestamp}.json'\nwith open(summary_file, 'w') as f:\n    json.dump(research_summary, f, indent=2)\nprint(f\"📋 Riassunto ricerca salvato in: {summary_file}\")\n\n# 3. Salva oggetti modello (per ulteriori analisi)\nmodels_file = output_dir / f'modelli_addestrati_{timestamp}.pkl'\nwith open(models_file, 'wb') as f:\n    pickle.dump(benchmark.results, f)\nprint(f\"🔬 Oggetti modello salvati in: {models_file}\")\n\n# 4. Crea report dettagliato\nreport_content = f\"\"\"# Report Ricerca Forecasting Serie Temporali\n\n**Generato:** {pd.Timestamp.now()}\n**Esperimenti:** {len(benchmark.results)}\n**Dataset:** {len(benchmark.datasets)}\n**Random Seed:** {RANDOM_SEED}\n\n## Sommario Esecutivo\n\n\"\"\"\n\nif len(results_df) > 0:\n    report_content += f\"\"\"\n### Risultati Chiave\n\n- **Miglior Algoritmo Complessivo:** {research_summary['risultati_chiave']['miglior_algoritmo_complessivo']} (RMSE: {research_summary['risultati_chiave']['miglior_performance_rmse']:.4f})\n- **Range Performance:** {research_summary['risultati_chiave']['range_performance']:.4f} unità RMSE\n- **Tempo Training Medio:** {research_summary['risultati_chiave']['tempo_training_medio']:.2f} secondi\n- **Utilizzo Memoria Medio:** {research_summary['risultati_chiave']['utilizzo_memoria_medio']:.1f} MB\n\n### Classifica Algoritmi (per RMSE)\n\n\"\"\"\n    for i, (algo, rmse) in enumerate(research_summary['classifica_algoritmi'].items(), 1):\n        report_content += f\"{i}. **{algo}**: {rmse:.4f}\\n\"\n    \n    report_content += \"\\n### Raccomandazioni\\n\\n\"\n    for rec in research_summary['raccomandazioni']:\n        report_content += f\"- {rec}\\n\"\n\nreport_content += f\"\"\"\n\n## Dataset Analizzati\n\n\"\"\"\n\nfor name, info in benchmark.datasets.items():\n    report_content += f\"- **{name}**: {info['length']} osservazioni - {info['description']}\\n\"\n\nreport_content += f\"\"\"\n\n## File Generati\n\n- `risultati_esperimenti_{timestamp}.csv`: Risultati sperimentali dettagliati\n- `riassunto_ricerca_{timestamp}.json`: Dati riassunto strutturati\n- `modelli_addestrati_{timestamp}.pkl`: Oggetti modello serializzati\n- `plots/`: File visualizzazioni\n\n---\n*Generato da Ambiente R&D ARIMA Forecaster*\n\"\"\"\n\nreport_file = output_dir / f'report_ricerca_{timestamp}.md'\nwith open(report_file, 'w') as f:\n    f.write(report_content)\nprint(f\"📄 Report ricerca salvato in: {report_file}\")\n\nprint(\"\\n🎉 SESSIONE DI RICERCA COMPLETATA!\")\nprint(\"=\" * 50)\nprint(f\"📁 Tutti gli output salvati in: {output_dir}\")\nprint(f\"📊 Esperimenti riusciti totali: {len(benchmark.results)}\")\nprint(f\"🎯 Algoritmo con migliori performance: {research_summary['risultati_chiave'].get('miglior_algoritmo_complessivo', 'N/A')}\")\n\nif len(results_df) > 0:\n    print(f\"📈 Range miglioramento performance: {research_summary['risultati_chiave']['range_performance']:.4f} unità RMSE\")\n\nprint(\"\\n💡 Prossimi Passi:\")\nprint(\"   1. Rivedi visualizzazioni e report generati\")\nprint(\"   2. Investiga ulteriormente algoritmi top-performing\")\nprint(\"   3. Considera metodi ensemble per produzione\")\nprint(\"   4. Testa su dataset aggiuntivi del mondo reale\")\nprint(\"   5. Implementa ottimizzazione iperparametri per migliori modelli\")\n\n# Mostra tabella riassunto finale\nif len(results_df) > 0:\n    print(\"\\n📊 RIASSUNTO PERFORMANCE FINALE:\")\n    summary_table = results_df.groupby('Algoritmo').agg({\n        'RMSE': ['mean', 'std'],\n        'R2': 'mean',\n        'Tempo_Training': 'mean'\n    }).round(4)\n    \n    summary_table.columns = ['RMSE_media', 'RMSE_std', 'R2_media', 'Tempo_media']\n    summary_table = summary_table.sort_values('RMSE_media')\n    \n    print(summary_table.to_string())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🎯 Conclusioni Ricerca e Direzioni Future\n\nQuesto notebook fornisce un ambiente R&D completo per la ricerca sul forecasting di serie temporali. Le capacità chiave includono:\n\n### ✅ **Funzionalità Implementate**\n\n1. **Confronto Sistematico Algoritmi**: Benchmarking equo attraverso algoritmi multipli\n2. **Valutazione Completa**: 10+ metriche di performance inclusi test statistici\n3. **Generazione Dataset Sintetici**: Esperimenti controllati con caratteristiche note\n4. **Analisi Importanza Feature**: Comprensione di cosa apprendono i modelli\n5. **Metodi Ensemble**: Combinazione modelli per performance migliorate\n6. **Visualizzazioni Professionali**: Grafici e analisi pronti per pubblicazione\n7. **Reporting Automatizzato**: Riassunti strutturati e raccomandazioni\n\n### 🔬 **Metodologia di Ricerca**\n\n- **Esperimenti Controllati**: Split train/test standardizzati e valutazione\n- **Tipi Dati Multipli**: Pattern lineari, esponenziali, rumorosi, stazionari\n- **Confronto Equo**: Stesso framework preprocessing e valutazione\n- **Rigore Statistico**: Metriche multiple e intervalli di confidenza\n- **Riproducibilità**: Random seed fissi e logging dettagliato\n\n### 🚀 **Miglioramenti Futuri**\n\n1. **Modelli Deep Learning**: Aggiungere architetture LSTM, GRU e Transformer\n2. **Ottimizzazione Iperparametri**: Tuning automatizzato con Optuna/Bayesiano\n3. **Cross-Validation**: Convalida incrociata serie temporali per valutazione robusta\n4. **Integrazione Dataset Reali**: Caricamento automatico dataset benchmark\n5. **Test Statistici**: Test Diebold-Mariano per significatività\n6. **Dashboard Interattiva**: Interfaccia Streamlit per sperimentazione real-time\n7. **Integrazione MLOps**: Versioning modelli e tracking esperimenti\n\n### 📈 **Linee Guida Utilizzo**\n\n1. **Fase Ricerca**: Usa questo notebook per esplorare nuovi algoritmi e capire il loro comportamento\n2. **Fase Sviluppo**: Prototipa nuovi approcci e valida concetti\n3. **Benchmarking**: Confronta i tuoi algoritmi contro baseline consolidate\n4. **Pubblicazione**: Usa visualizzazioni e risultati generati in paper di ricerca\n\n---\n\n**Questo ambiente R&D consente ricerca sistematica, riproducibile e ricca di insight sul forecasting di serie temporali. Modificalo ed estendilo in base alle tue specifiche esigenze di ricerca!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}